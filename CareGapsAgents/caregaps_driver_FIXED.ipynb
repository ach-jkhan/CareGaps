{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4dceb2-3f65-4b41-87f2-87a9ec52fa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Tool-calling Agent\n",
    "\n",
    "This is an auto-generated notebook created by an AI playground export. In this notebook, you will:\n",
    "- Author a tool-calling [MLflow's `ResponsesAgent`](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.pyfunc.html#mlflow.pyfunc.ResponsesAgent) that uses the OpenAI client\n",
    "- Manually test the agent's output\n",
    "- Evaluate the agent with Mosaic AI Agent Evaluation\n",
    "- Log and deploy the agent\n",
    "\n",
    "This notebook should be run on serverless or a cluster with DBR<17.\n",
    "\n",
    " **_NOTE:_**  This notebook uses the OpenAI SDK, but AI Agent Framework is compatible with any agent authoring framework, including LlamaIndex or LangGraph. To learn more, see the [Authoring Agents](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/author-agent) Databricks documentation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Address all `TODO`s in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fc8f89-bedd-400e-9270-1489a5f27728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-openai uv databricks-agents mlflow-skinny[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00cfc352-fc0b-41de-aaa1-e19abd993022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the agent in code\n",
    "Below we define our agent code in a single cell, enabling us to easily write it to a local Python file for subsequent logging and deployment using the `%%writefile` magic command.\n",
    "\n",
    "For more examples of tools to add to your agent, see [docs](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/agent-tool)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "import-agent-markdown",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Agent\n",
    "Import the agent we just created in agent.py. This step is required before testing or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82619afa-d206-4586-87ef-efa9258f51e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Callable, Generator, Optional\n",
    "from uuid import uuid4\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import openai\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_openai import UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from unitycatalog.ai.core.base import get_uc_function_client\n",
    "\n",
    "\n",
    "############################################\n",
    "# Configuration\n",
    "############################################\n",
    "#LLM_ENDPOINT_NAME = \"databricks-gpt-oss-20b\"\n",
    "LLM_ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "# System Prompt - Example-Driven for Llama 3.3 70B\n",
    "SYSTEM_PROMPT = \"\"\"You are the CareGaps Assistant for Akron Children's Hospital. Your role is to help clinicians, care coordinators, and administrators query and analyze patient care gaps AND outreach campaigns using natural language.\n",
    "\n",
    "CAPABILITIES:\n",
    "You have access to 19 SQL functions:\n",
    "\n",
    "**Care Gaps Analysis (15 functions):**\n",
    "- Patient-specific queries (search, view gaps, 360-degree view)\n",
    "- Priority and urgency queries (critical gaps, long-open gaps, outreach needs, no appointments)\n",
    "- Provider and department analysis\n",
    "- Statistical overviews and trends\n",
    "- Appointment coordination\n",
    "- Gap type and category analysis\n",
    "\n",
    "**Campaign Analytics (4 functions):**\n",
    "- Campaign statistics and metrics\n",
    "- Search campaign opportunities by patient, location, or MRN\n",
    "- List and filter campaign opportunities\n",
    "- Patient campaign history\n",
    "\n",
    "DATA SCOPE:\n",
    "- Pediatric patients with active care gaps\n",
    "- Gap types: Immunizations, Well Child Visits, BMI Screenings, Developmental Assessments, etc.\n",
    "- Priority levels: Critical, Important, Routine\n",
    "- Provider assignments and departments\n",
    "- Appointment scheduling information\n",
    "- Patient contact information (phone, email)\n",
    "- **Flu Vaccine Piggybacking Campaign:** Identifies siblings who need flu vaccines and can piggyback on a household member's existing appointment\n",
    "\n",
    "CAMPAIGN CONTEXT — FLU VACCINE PIGGYBACKING:\n",
    "This is an agentic AI campaign that identifies TRUE piggybacking opportunities:\n",
    "- A \"subject patient\" has an upcoming appointment\n",
    "- A sibling in the same household is overdue for their flu vaccine but has NO appointment of their own\n",
    "- The system suggests: \"Bring sibling for their flu shot while you're here for the appointment\"\n",
    "- Siblings who already have their own appointments are EXCLUDED (this is the AI differentiator)\n",
    "- Campaign types: FLU_VACCINE (active), LAB_PIGGYBACKING and DEPRESSION_SCREENING (coming soon)\n",
    "- Statuses: pending → approved → sent → completed\n",
    "\n",
    "IMPORTANT — CHAT vs DASHBOARD BOUNDARY:\n",
    "This chat agent handles ANALYTICAL and READ-ONLY queries only.\n",
    "Campaign operations (approve, send messages, change status) belong in the **Flu Campaign Dashboard**.\n",
    "If a user asks to \"send a message\", \"approve this opportunity\", or \"mark as completed\":\n",
    "→ Respond: \"That action is available in the Campaign Dashboard. Navigate to **Campaigns → Flu Vaccine** in the sidebar to review, approve, and send messages.\"\n",
    "\n",
    "SCOPE BOUNDARY:\n",
    "You ONLY answer questions related to pediatric care gaps, patient outreach, campaigns, flu vaccine piggybacking, and Akron Children's Hospital clinical operations.\n",
    "If a user asks about anything unrelated (recipes, general knowledge, coding, weather, etc.), politely decline:\n",
    "→ \"I'm the CareGaps Assistant and can only help with care gap analysis, outreach campaigns, and patient data for Akron Children's Hospital. How can I help you with care gaps today?\"\n",
    "\n",
    "RESPONSE GUIDELINES:\n",
    "1. ALWAYS provide specific, actionable information\n",
    "2. Format results as markdown tables with | separators\n",
    "3. ALWAYS include \"Next Best Actions\" or \"Recommendations\" section\n",
    "4. Show ALL rows returned - never truncate results\n",
    "5. Prioritize critical gaps over routine ones\n",
    "6. Suggest relevant follow-up questions\n",
    "7. Be concise but complete\n",
    "\n",
    "EXAMPLE INTERACTIONS:\n",
    "\n",
    "User: \"Show me critical gaps\"\n",
    "You: [Call get_critical_gaps(limit_rows=100)]\n",
    "     \"Here are the critical priority care gaps requiring immediate attention:\n",
    "\n",
    "     | Patient Name | MRN | Age | Gap Type | Days Open | PCP | Phone | Next Appt |\n",
    "     |---|---|---|---|---|---|---|---|\n",
    "     | Smith, John | ***5678 | 5 | Immunization | 120 | Dr. Jones | ***-0123 | None |\n",
    "     ...\n",
    "\n",
    "     ### Next Best Actions:\n",
    "     • Patients with no upcoming appointments need priority outreach\n",
    "     • Gaps open >90 days should be escalated\n",
    "     • Consider group vaccination clinic for immunization gaps\"\n",
    "\n",
    "User: \"How is the flu campaign going?\"\n",
    "You: [Call get_campaign_statistics(campaign_type_filter='FLU_VACCINE')]\n",
    "     \"Here are the current flu vaccine piggybacking campaign metrics:\n",
    "\n",
    "     | Metric | Value |\n",
    "     |---|---|\n",
    "     | Total Opportunities | 8,234 |\n",
    "     | Pending Review | 5,102 |\n",
    "     | Approved | 2,045 |\n",
    "     | Sent | 987 |\n",
    "     | Completed | 100 |\n",
    "     | Asthma Patients (J45) | 412 |\n",
    "     ...\n",
    "\n",
    "     ### Next Best Actions:\n",
    "     • 5,102 opportunities still pending review — head to the Campaign Dashboard to approve\n",
    "     • 412 asthma patients should be prioritized (higher flu risk)\n",
    "     • Focus on HIGH confidence matches first for best outreach ROI\"\n",
    "\n",
    "User: \"Show flu opportunities at Beachwood\"\n",
    "You: [Call get_campaign_opportunities(campaign_type_filter='FLU_VACCINE', status_filter='', location_filter='Beachwood', limit_rows=50)]\n",
    "     \"Here are the flu vaccine piggybacking opportunities at Beachwood:\n",
    "\n",
    "     | Patient | MRN | Age | Relationship | Subject | Appt Date | Asthma | Status |\n",
    "     |---|---|---|---|---|---|---|---|\n",
    "     | Doe, Sarah | ***1234 | 4 | Shared Address | Doe, Tommy (***5678) | 2026-02-20 | N | pending |\n",
    "     ...\n",
    "\n",
    "     ### Next Best Actions:\n",
    "     • Review and approve these in the Campaign Dashboard\n",
    "     • Prioritize asthma patients for outreach\n",
    "     • Check if any siblings share the same appointment date for batch processing\"\n",
    "\n",
    "User: \"Send a message to this patient\"\n",
    "You: \"That action is available in the Campaign Dashboard. Navigate to **Campaigns → Flu Vaccine** in the sidebar to review, approve, and send messages.\"\n",
    "\n",
    "User: \"Find patient John Smith\"\n",
    "You: [Call search_patients(search_term='John Smith')]\n",
    "     Return matching patients with gap summary, suggest get_patient_360() for details.\n",
    "\n",
    "User: \"Any asthma siblings in the flu campaign?\"\n",
    "You: [Call get_campaign_opportunities(campaign_type_filter='FLU_VACCINE', status_filter='', location_filter='', limit_rows=100)]\n",
    "     Filter and highlight rows where has_asthma = 'Y', recommend prioritizing these for outreach.\n",
    "\n",
    "FUNCTION SELECTION (19 functions):\n",
    "\n",
    "**Care Gaps (15):**\n",
    "- Patient search/find → search_patients()\n",
    "- Patient gaps → get_patient_gaps()\n",
    "- Comprehensive/360/everything about patient → get_patient_360()\n",
    "- Critical/urgent gaps → get_critical_gaps()\n",
    "- Long-open gaps → get_long_open_gaps()\n",
    "- Outreach needed → get_outreach_needed()\n",
    "- Gaps with NO appointments → get_gaps_no_appointments()\n",
    "- Provider/department gaps → get_provider_gaps()\n",
    "- Department summary → get_department_summary()\n",
    "- Top providers → get_top_providers()\n",
    "- Gap statistics → get_gap_statistics()\n",
    "- Gaps by type → get_gaps_by_type()\n",
    "- Gaps by age → get_gaps_by_age()\n",
    "- Gap categories → get_gap_categories()\n",
    "- Appointments with gaps → get_appointments_with_gaps()\n",
    "\n",
    "**Campaigns (4):**\n",
    "- Campaign stats/metrics/overview → get_campaign_statistics(campaign_type_filter)\n",
    "- Search by MRN/name/location → search_campaign_opportunities(search_term, campaign_type_filter)\n",
    "- List/filter opportunities → get_campaign_opportunities(campaign_type_filter, status_filter, location_filter, limit_rows)\n",
    "- Patient campaign history → get_patient_campaign_history(patient_mrn_filter)\n",
    "\n",
    "CAMPAIGN TYPE VALUES:\n",
    "- \"FLU_VACCINE\" — Flu vaccine piggybacking (active)\n",
    "- \"LAB_PIGGYBACKING\" — Lab piggybacking (coming soon)\n",
    "- \"DEPRESSION_SCREENING\" — Depression screening PHQ-9 (coming soon)\n",
    "\n",
    "When user mentions \"flu\", \"flu vaccine\", \"flu campaign\", \"piggybacking\" → use campaign_type_filter = \"FLU_VACCINE\"\n",
    "\n",
    "CONTEXT MAINTENANCE:\n",
    "- Remember conversation history\n",
    "- When user says \"this patient\" or \"that patient\", refer to the most recently mentioned patient\n",
    "- When user asks for \"more information\" about a patient just shown, use get_patient_360() with that patient's ID\n",
    "\n",
    "CRITICAL:\n",
    "- ALWAYS format results as markdown tables with | separators\n",
    "- NEVER return raw comma-separated data\n",
    "- ALWAYS include \"### Next Best Actions:\" section after data\n",
    "- SHOW ALL ROWS - never truncate to 3 or 10 results\n",
    "- For campaign operations (approve, send, update status) → redirect to Campaign Dashboard\"\"\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Logging and Monitoring\n",
    "###############################################################################\n",
    "\n",
    "class AgentLogger:\n",
    "    \"\"\"Log agent interactions for monitoring and debugging\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_query(user_query: str, functions_called: list[str], success: bool, error: str = None):\n",
    "        \"\"\"Log query to MLflow or database\"\"\"\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": user_query,\n",
    "            \"functions\": functions_called,\n",
    "            \"success\": success,\n",
    "            \"error\": error,\n",
    "            \"model\": LLM_ENDPOINT_NAME\n",
    "        }\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_dict(log_entry, f\"query_{datetime.now().timestamp()}.json\")\n",
    "        \n",
    "        # Print for debugging (remove in production)\n",
    "        print(f\"[AGENT LOG] {json.dumps(log_entry)}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_error(error_type: str, error_message: str, context: dict = None):\n",
    "        \"\"\"Log errors for debugging\"\"\"\n",
    "        error_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": error_type,\n",
    "            \"message\": error_message,\n",
    "            \"context\": context or {}\n",
    "        }\n",
    "        \n",
    "        mlflow.log_dict(error_entry, f\"error_{datetime.now().timestamp()}.json\")\n",
    "        print(f\"[ERROR] {json.dumps(error_entry)}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Input Validation\n",
    "###############################################################################\n",
    "\n",
    "class InputValidator:\n",
    "    \"\"\"Validate user inputs to prevent injection attacks\"\"\"\n",
    "    \n",
    "    # Dangerous patterns that might indicate SQL injection attempts\n",
    "    DANGEROUS_PATTERNS = [\n",
    "        r\";\\s*drop\\s+table\",\n",
    "        r\";\\s*delete\\s+from\",\n",
    "        r\";\\s*update\\s+.*\\s+set\",\n",
    "        r\"union\\s+select\",\n",
    "        r\"--\\s*$\",\n",
    "        r\"/\\*.*\\*/\",\n",
    "    ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_safe_input(user_input: str) -> tuple[bool, str]:\n",
    "        \"\"\"Check if user input is safe\"\"\"\n",
    "        if not user_input:\n",
    "            return False, \"Empty input\"\n",
    "        \n",
    "        # Check length\n",
    "        if len(user_input) > 1000:\n",
    "            return False, \"Input too long (max 1000 characters)\"\n",
    "        \n",
    "        # Check for dangerous SQL patterns\n",
    "        for pattern in InputValidator.DANGEROUS_PATTERNS:\n",
    "            if re.search(pattern, user_input, re.IGNORECASE):\n",
    "                return False, f\"Potentially dangerous input detected\"\n",
    "        \n",
    "        return True, \"Valid\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_input(user_input: str) -> str:\n",
    "        \"\"\"Sanitize user input\"\"\"\n",
    "        # Remove any control characters\n",
    "        sanitized = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', user_input)\n",
    "        \n",
    "        # Trim whitespace\n",
    "        sanitized = sanitized.strip()\n",
    "        \n",
    "        return sanitized\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Tool Definition\n",
    "###############################################################################\n",
    "\n",
    "class ToolInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    Class representing a tool for the agent.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "\n",
    "def create_tool_info(tool_spec, exec_fn_param: Optional[Callable] = None):\n",
    "    tool_spec[\"function\"].pop(\"strict\", None)\n",
    "    tool_name = tool_spec[\"function\"][\"name\"]\n",
    "    udf_name = tool_name.replace(\"__\", \".\")\n",
    "\n",
    "    def exec_fn(**kwargs):\n",
    "        \"\"\"Execute UC function with error handling and PHI masking\"\"\"\n",
    "        try:\n",
    "            # Execute function\n",
    "            function_result = uc_function_client.execute_function(udf_name, kwargs)\n",
    "            \n",
    "            if function_result.error is not None:\n",
    "                AgentLogger.log_error(\n",
    "                    \"function_execution_error\",\n",
    "                    function_result.error,\n",
    "                    {\"function\": udf_name, \"kwargs\": kwargs}\n",
    "                )\n",
    "                return f\"Error executing {udf_name}: {function_result.error}\"\n",
    "            \n",
    "            return function_result.value\n",
    "            \n",
    "        except Exception as e:\n",
    "            AgentLogger.log_error(\n",
    "                \"function_exception\",\n",
    "                str(e),\n",
    "                {\"function\": udf_name, \"kwargs\": kwargs}\n",
    "            )\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ToolInfo(name=tool_name, spec=tool_spec, exec_fn=exec_fn_param or exec_fn)\n",
    "\n",
    "\n",
    "# Configure UC Functions\n",
    "UC_TOOL_NAMES = [\n",
    "    # Care Gaps (15 functions)\n",
    "    \"dev_kiddo.silver.get_top_providers\",\n",
    "    \"dev_kiddo.silver.get_patient_360\",\n",
    "    \"dev_kiddo.silver.get_gap_categories\",\n",
    "    \"dev_kiddo.silver.get_provider_gaps\",\n",
    "    \"dev_kiddo.silver.get_long_open_gaps\",\n",
    "    \"dev_kiddo.silver.get_outreach_needed\",\n",
    "    \"dev_kiddo.silver.get_appointments_with_gaps\",\n",
    "    \"dev_kiddo.silver.get_critical_gaps\",\n",
    "    \"dev_kiddo.silver.search_patients\",\n",
    "    \"dev_kiddo.silver.get_gaps_by_type\",\n",
    "    \"dev_kiddo.silver.get_gap_statistics\",\n",
    "    \"dev_kiddo.silver.get_department_summary\",\n",
    "    \"dev_kiddo.silver.get_gaps_by_age\",\n",
    "    \"dev_kiddo.silver.get_gaps_no_appointments\",\n",
    "    \"dev_kiddo.silver.get_patient_gaps\",\n",
    "    # Campaign Analytics (4 functions)\n",
    "    \"dev_kiddo.silver.get_campaign_statistics\",\n",
    "    \"dev_kiddo.silver.search_campaign_opportunities\",\n",
    "    \"dev_kiddo.silver.get_campaign_opportunities\",\n",
    "    \"dev_kiddo.silver.get_patient_campaign_history\",\n",
    "]\n",
    "\n",
    "TOOL_INFOS = []\n",
    "\n",
    "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES)\n",
    "uc_function_client = get_uc_function_client()\n",
    "\n",
    "for tool_spec in uc_toolkit.tools:\n",
    "    TOOL_INFOS.append(create_tool_info(tool_spec))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Agent Implementation\n",
    "###############################################################################\n",
    "\n",
    "class ToolCallingAgent(ResponsesAgent):\n",
    "    \"\"\"Enhanced tool-calling Agent with PHI protection\"\"\"\n",
    "\n",
    "    def __init__(self, llm_endpoint: str, tools: list[ToolInfo]):\n",
    "        \"\"\"Initializes the ToolCallingAgent with tools.\"\"\"\n",
    "        self.llm_endpoint = llm_endpoint\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.model_serving_client: OpenAI = (\n",
    "            self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        )\n",
    "        self._tools_dict = {tool.name: tool for tool in tools}\n",
    "        self._functions_called = []  # Track function calls for logging\n",
    "\n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        \"\"\"Returns tool specifications in the format OpenAI expects.\"\"\"\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        \"\"\"Executes the specified tool with the given arguments.\"\"\"\n",
    "        self._functions_called.append(tool_name)\n",
    "    \n",
    "        # Execute the tool\n",
    "        result = self._tools_dict[tool_name].exec_fn(**args)\n",
    "    \n",
    "         # ⭐ Format results instead of returning raw\n",
    "        if isinstance(result, dict):\n",
    "            formatted = self._format_dict_result(result)\n",
    "        elif isinstance(result, list):\n",
    "            formatted = self._format_list_result(result)\n",
    "        else:\n",
    "            formatted = str(result)\n",
    "        \n",
    "        # ✅ Add instruction for LLM to provide next steps\n",
    "        # Apply to both lists (patient data) AND dicts (statistics)\n",
    "        if isinstance(result, (list, dict)) and result:\n",
    "            formatted += \"\\n\\n[INSTRUCTION: After presenting this data, you MUST add a '### Next Best Actions:' section with 3-5 specific, actionable recommendations based on this data. Be concrete and clinical in your recommendations.]\"\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        \"\"\"Call LLM with error handling\"\"\"\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "                for chunk in self.model_serving_client.chat.completions.create(\n",
    "                    model=self.llm_endpoint,\n",
    "                    messages=to_chat_completions_input(messages),\n",
    "                    tools=self.get_tool_specs(),\n",
    "                    stream=True,\n",
    "                    temperature=0.0,  # Lower temperature for more consistent function calling\n",
    "                    max_tokens=4096,\n",
    "                ):\n",
    "                    chunk_dict = chunk.to_dict()\n",
    "                    if len(chunk_dict.get(\"choices\", [])) > 0:\n",
    "                        yield chunk_dict\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "\n",
    "            AgentLogger.log_error(\"llm_call_error\", error_msg)\n",
    "            # Yield error message as text response\n",
    "            yield {\n",
    "                \"choices\": [{\n",
    "                    \"delta\": {\n",
    "                        \"content\": f\"I'm sorry, I encountered an error processing your request. Please try again.\"\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "\n",
    "    def handle_tool_call(\n",
    "        self,\n",
    "        tool_call: dict[str, Any],\n",
    "        messages: list[dict[str, Any]],\n",
    "    ) -> ResponsesAgentStreamEvent:\n",
    "        \"\"\"Execute tool calls with error handling\"\"\"\n",
    "        try:\n",
    "            raw_name = tool_call[\"name\"]\n",
    "            clean_name = self._sanitize_function_name(raw_name)\n",
    "\n",
    "            args = json.loads(tool_call[\"arguments\"])\n",
    "\n",
    "            if isinstance(args, dict):\n",
    "                # Remove empty keys (LLM sometimes generates {\"\": \"\"})\n",
    "                args = {k: v for k, v in args.items() if k and k.strip()}\n",
    "        \n",
    "            # ADD THIS: If args is now empty dict, check if function needs params\n",
    "            if not args:\n",
    "                # Check if function has required parameters\n",
    "                tool_info = self._tools_dict.get(clean_name)\n",
    "                if tool_info and hasattr(tool_info, 'parameters'):\n",
    "                    # If function has required params but we have none, that's an error\n",
    "                    required_params = getattr(tool_info.parameters, 'required', [])\n",
    "                    if required_params:\n",
    "                        print(f\"[ERROR] Function '{clean_name}' requires params: {required_params}\")\n",
    "                        result = f\"Error: This function requires parameters. Please provide: {', '.join(required_params)}\"\n",
    "                        # Skip to the end\n",
    "                        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "                        messages.append(tool_call_output)\n",
    "                        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "            if clean_name not in self._tools_dict:\n",
    "                print(f\"[ERROR] Function '{clean_name}' not found.\")\n",
    "                print(f\"[Error] Available: {list(self._tools_dict.keys())[:3]}...\")\n",
    "                result = f\"Error: Function not found. Please rephrase your query.\"\n",
    "            else:\n",
    "                result = str(self.execute_tool(tool_name=clean_name, args=args))\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            AgentLogger.log_error(\n",
    "                \"tool_call_error\",\n",
    "                str(e),\n",
    "                {\"tool\": tool_call[\"name\"], \"args\": tool_call.get(\"arguments\")}\n",
    "            )\n",
    "            result = f\"Error executing tool: {str(e)}\"\n",
    "\n",
    "        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "        messages.append(tool_call_output)\n",
    "        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "    def call_and_run_tools(\n",
    "    self,\n",
    "    messages: list[dict[str, Any]],\n",
    "    max_iter: int = 10,  # ⭐ Increased back to 10\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Call LLM and execute tools with iteration limit\"\"\"\n",
    "    \n",
    "        # ⭐ ADD THIS: Limit conversation history to prevent context overflow\n",
    "        if len(messages) > 7:\n",
    "            system_prompt = messages[0] if messages[0].get('role') == 'system' else None\n",
    "            recent_messages = messages[-6:]\n",
    "            \n",
    "            if system_prompt:\n",
    "                messages = [system_prompt] + recent_messages\n",
    "            else:\n",
    "                messages = recent_messages\n",
    "\n",
    "            print(f\"[Debug] Trimmed to {len(messages)} messages\")\n",
    "    \n",
    "        # Continue with existing loop\n",
    "        for iteration in range(max_iter):\n",
    "            last_msg = messages[-1]\n",
    "            if last_msg.get(\"role\", None) == \"assistant\":\n",
    "                return\n",
    "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
    "                yield self.handle_tool_call(last_msg, messages)\n",
    "            else:\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    chunks=self.call_llm(messages), aggregator=messages\n",
    "                )\n",
    "\n",
    "        # Max iterations reached\n",
    "        AgentLogger.log_error(\"max_iterations\", f\"Reached max iterations ({max_iter})\")\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\n",
    "                \"I apologize, but I'm having trouble completing this request. Please try rephrasing or breaking it into simpler questions.\",\n",
    "                str(uuid4())\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        \"\"\"Generate a response for the given request\"\"\"\n",
    "    \n",
    "        # Generate response using predict_stream\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "    \n",
    "        # Handle custom_inputs for both formats\n",
    "        custom_outputs = None\n",
    "        if isinstance(request, dict):\n",
    "            custom_outputs = request.get('custom_inputs', None)\n",
    "        elif hasattr(request, 'custom_inputs'):\n",
    "            custom_outputs = request.custom_inputs\n",
    "    \n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=custom_outputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self, request: ResponsesAgentRequest\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        \"\"\"Stream prediction with PHI warning\"\"\"\n",
    "    \n",
    "        # ⭐ Handle both dict and ResponsesAgentRequest formats\n",
    "        if isinstance(request, dict):\n",
    "            # Dict format\n",
    "            messages = request.get('input', [])\n",
    "        elif hasattr(request, 'input'):\n",
    "            # ResponsesAgentRequest format\n",
    "            if hasattr(request.input[0], 'model_dump'):\n",
    "                messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "            else:\n",
    "                messages = to_chat_completions_input(request.input)\n",
    "        else:\n",
    "            messages = []\n",
    "    \n",
    "        if SYSTEM_PROMPT:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    \n",
    "        # Generate responses\n",
    "        yield from self.call_and_run_tools(messages=messages)\n",
    "    \n",
    "    def _call_agent(self, request: ResponsesAgentRequest) -> Generator:\n",
    "        \"\"\"Internal method to call agent with proper message handling\"\"\"\n",
    "        messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "    \n",
    "        if SYSTEM_PROMPT:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "    \n",
    "        yield from self.call_and_run_tools(messages=messages)\n",
    "    \n",
    "    def _format_dict_result(self, result: dict) -> str:\n",
    "        \"\"\"Format dictionary result as readable text\"\"\"\n",
    "        lines = []\n",
    "        for key, value in result.items():\n",
    "            readable_key = key.replace('_', ' ').title()\n",
    "            lines.append(f\"{readable_key}: {value}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _format_list_result(self, result: list) -> str:\n",
    "        \"\"\"Format list result as table or bullets\"\"\"\n",
    "        if not result:\n",
    "            return \"No results found.\"\n",
    "        \n",
    "        if isinstance(result[0], dict):\n",
    "            return self._format_table(result)\n",
    "        else:\n",
    "            return \"\\n\".join(f\"• {item}\" for item in result)\n",
    "\n",
    "\n",
    "    def _format_table(self, data: list) -> str:\n",
    "        \"\"\"Format list of dicts as a markdown table\"\"\"\n",
    "        if not data:\n",
    "            return \"No results found.\"\n",
    "        \n",
    "        headers = list(data[0].keys())\n",
    "        readable_headers = [h.replace('_', ' ').title() for h in headers]\n",
    "        \n",
    "        lines = []\n",
    "        lines.append(\"| \" + \" | \".join(readable_headers) + \" |\")  # Proper markdown\n",
    "        lines.append(\"|\" + \"|\".join([\"---\" for _ in headers]) + \"|\")  # Proper separator\n",
    "        \n",
    "        for row in data:\n",
    "            # Truncate long cell values to 80 chars to keep tables readable\n",
    "            values = [str(row.get(h, ''))[:80] for h in headers]\n",
    "            lines.append(\"| \" + \" | \".join(values) + \" |\")\n",
    "        \n",
    "        # Add total count\n",
    "        lines.append(f\"\\n**Total: {len(data)} results**\")\n",
    "        lines.append(\"\\n### Next Best Actions:\")\n",
    "        lines.append(\"Please provide 3-5 specific action items based on this data.\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _sanitize_function_name(self, raw_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove hallucinated tokens from function names.\n",
    "        Fixes: dev_kiddo__silver__get_statistics<|channel|>commentary\n",
    "        \"\"\"\n",
    "        if not raw_name:\n",
    "            return raw_name\n",
    "        \n",
    "        # Known hallucination tokens\n",
    "        bad_tokens = [\n",
    "            '<|channel|>',\n",
    "            '<|commentary|>',\n",
    "            'commentary',\n",
    "            'channel',\n",
    "            '<|',\n",
    "            '|>',\n",
    "        ]\n",
    "        \n",
    "        sanitized = raw_name\n",
    "        for token in bad_tokens:\n",
    "            sanitized = sanitized.replace(token, '')\n",
    "        \n",
    "        # Log if we had to clean\n",
    "        if sanitized != raw_name:\n",
    "            print(f\"[SANITIZED] {raw_name} → {sanitized}\")\n",
    "        \n",
    "        return sanitized\n",
    "    \n",
    "###############################################################################\n",
    "## Model Logging\n",
    "###############################################################################\n",
    "\n",
    "# Log the model using MLflow\n",
    "mlflow.openai.autolog()\n",
    "AGENT = ToolCallingAgent(llm_endpoint=LLM_ENDPOINT_NAME, tools=TOOL_INFOS)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "import-agent-cell",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Agent"
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# SIMPLE WORKING TEST\n# =====================================================\n\nimport mlflow\nfrom agent import AGENT\n\n# Close any active MLflow runs\nwhile mlflow.active_run():\n    print(f\"Closing active run: {mlflow.active_run().info.run_id}\")\n    mlflow.end_run()\n\nprint(\"✓ All MLflow runs closed\\n\")\n\n# Test 1: Normal query\nprint(\"=\"*60)\nprint(\"TEST 1: Normal query\")\nprint(\"=\"*60)\n\ntry:\n    r1 = AGENT.predict({\n        \"input\": [{\"role\": \"user\", \"content\": \"Show me 3 critical gaps\"}]\n    })\n    \n    response_text = str(r1)\n    print(f\"✓ Response received ({len(response_text)} chars)\")\n    print(f\"  Preview: {response_text[:200]}...\")\n    print(\"✓✓ TEST 1 PASSED\")\n        \nexcept Exception as e:\n    print(f\"✗ Error: {e}\")\n    import traceback\n    traceback.print_exc()\n\n# Test 2: Campaign query\nprint(\"\\n\" + \"=\"*60)\nprint(\"TEST 2: Campaign query\")\nprint(\"=\"*60)\n\ntry:\n    r2 = AGENT.predict({\n        \"input\": [{\"role\": \"user\", \"content\": \"How is the flu campaign going?\"}]\n    })\n    \n    response_text = str(r2)\n    print(f\"✓ Response received ({len(response_text)} chars)\")\n    print(f\"  Preview: {response_text[:200]}...\")\n    print(\"✓✓ TEST 2 PASSED\")\n        \nexcept Exception as e:\n    print(f\"✗ Error: {e}\")\n    import traceback\n    traceback.print_exc()\n\n# Test 3: Multiple queries (test non-responsiveness fix)\nprint(\"\\n\" + \"=\"*60)\nprint(\"TEST 3: Multiple consecutive queries\")\nprint(\"=\"*60)\n\nsuccess_count = 0\nfor i in range(5):\n    try:\n        r = AGENT.predict({\n            \"input\": [{\"role\": \"user\", \"content\": \"How many gaps?\"}]\n        })\n        print(f\"Query {i+1}: ✓ Success\")\n        success_count += 1\n    except Exception as e:\n        print(f\"Query {i+1}: ✗ Failed - {str(e)[:100]}\")\n\nif success_count == 5:\n    print(\"✓✓ TEST 3 PASSED - Agent handled 5 consecutive queries\")\nelse:\n    print(f\"✗✗ TEST 3 FAILED - Only {success_count}/5 queries succeeded\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64978ddb-1769-433e-ba94-fc249a0375e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test the agent\n",
    "\n",
    "Interact with the agent to test its output. Since we manually traced methods within `ResponsesAgent`, you can view the trace for each step the agent takes, with any LLM calls made via the OpenAI SDK automatically traced by autologging.\n",
    "\n",
    "Replace this placeholder input with an appropriate domain-specific example for your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765563de-d0ef-4f21-9259-bb17257d433b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT\n",
    "\n",
    "# Test 1: Simple query\n",
    "r1 = AGENT.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"How many gaps?\"}]\n",
    "})\n",
    "print(f\"Length: {len(str(r1))}\")  # Should have data\n",
    "\n",
    "# Test 2: Patient search\n",
    "r2 = AGENT.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Find patient 2886348\"}]\n",
    "})\n",
    "result = str(r2)\n",
    "print(f\"Has MRN: {'2886348' in result}\")  # Should be True (unmasked!)\n",
    "print(f\"Has table: {'|' in result}\")  # Should be True (formatted)\n",
    "\n",
    "# Test 3: Multi-step query\n",
    "r3 = AGENT.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Find patient 2886348 and show their gaps\"}]\n",
    "})\n",
    "print(f\"Result length: {len(str(r3))}\")  # Should have substantial data\n",
    "\n",
    "print(\"✓ All tests passed! Agent returns clean, unmasked data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a12ecb7-d97c-458a-bfd6-07dd0974a4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick test in notebook\n",
    "from agent import AGENT\n",
    "\n",
    "print(\"Testing consecutive queries (where it used to stall)...\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\nQuery {i+1}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        response = AGENT.predict({\n",
    "            \"input\": [{\"role\": \"user\", \"content\": \"Show me gap statistics\"}]\n",
    "        })\n",
    "        \n",
    "        output_len = len(str(response))\n",
    "        print(f\"OK ({output_len} chars)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ FAILED: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n✅ Test complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1683511c-a3f5-412b-9c79-e85b0722402a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# DIAGNOSTIC: CSV Parsing (Optional)\n# =====================================================\n# Note: PHIMasker was removed from the agent.\n# This cell is kept for reference but will not run as-is.\nprint(\"Skipped - PHIMasker diagnostic cell (no longer applicable)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d77b3f2-2ab7-48b3-bba0-e80e62adb39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# DIAGNOSTIC: UC Function Return Type (Optional)\n# =====================================================\n\nfrom unitycatalog.ai.core.base import get_uc_function_client\n\nprint(\"=\"*70)\nprint(\"DIAGNOSTIC: UC Function Return Type Analysis\")\nprint(\"=\"*70)\n\nuc_client = get_uc_function_client()\n\nprint(\"\\nTesting UC function directly...\")\ntry:\n    result = uc_client.execute_function(\n        \"dev_kiddo.silver.get_critical_gaps\",\n        {\"limit_rows\": 3}\n    )\n    \n    print(f\"Result object type: {type(result)}\")\n    \n    if hasattr(result, 'value'):\n        print(f\"Result.value type: {type(result.value)}\")\n        print(f\"Result.value preview: {str(result.value)[:300]}\")\n    \n    if hasattr(result, 'error') and result.error:\n        print(f\"Result.error: {result.error}\")\n    else:\n        print(\"✓ No errors\")\n        \nexcept Exception as e:\n    print(f\"Error: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n✓ DIAGNOSTIC COMPLETE\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7a3459f2-ef53-4b08-9f5a-6008ca495ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# Quick agent test\n# =====================================================\nfrom agent import AGENT\n\nresponse = AGENT.predict({\n    \"input\": [{\"role\": \"user\", \"content\": \"Show me gap statistics\"}]\n})\nprint(f\"Response length: {len(str(response))} chars\")\nprint(f\"Preview: {str(response)[:300]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247b21b6-0ab0-4de4-95eb-5a089eb6cc8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# Quick model registration (alternative to cell 14+21)\n# =====================================================\n# Note: Use cells 14 and 21 for the full logging + registration flow.\n# This cell is a shortcut if you already have AGENT loaded and tested.\n\nimport mlflow\nfrom agent import AGENT\n\nif mlflow.active_run():\n    mlflow.end_run()\n\ntest_request = {\"input\": [{\"role\": \"user\", \"content\": \"How many gaps?\"}]}\ntest_response = AGENT.predict(test_request)\n\nwith mlflow.start_run():\n    mlflow.models.log_model(\n        name=\"agent\",\n        python_model=AGENT,\n        input_example=test_request,\n        signature=mlflow.models.infer_signature(test_request, test_response)\n    )\n    print(\"✓ Model logged\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f35bd94-cd2e-4d4c-81f4-4f2a84a0cadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Log the `agent` as an MLflow model\n",
    "Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "- **TODO**: If your Unity Catalog Function queries a [vector search index](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/unstructured-retrieval-tools) or leverages [external functions](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/external-connection-tools), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See [docs](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#specify-resources-for-automatic-authentication-passthrough) for more details.\n",
    "\n",
    "Log the agent as code from the `agent.py` file. See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1303d93d-c06b-4811-a35f-5d194bf14243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\nimport mlflow\nfrom agent import UC_TOOL_NAMES, LLM_ENDPOINT_NAME\nfrom mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n\nresources = [DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME)]\nfor tool_name in UC_TOOL_NAMES:\n    resources.append(DatabricksFunction(function_name=tool_name))\n\ninput_example = {\n    \"input\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"what can you help me with?\"\n        }\n    ]\n}\n\nif mlflow.active_run():\n    print(f\"⚠ Ending previous run: {mlflow.active_run().info.run_id}\")\n    mlflow.end_run()\n\nwith mlflow.start_run():\n    logged_agent_info = mlflow.pyfunc.log_model(\n        name=\"agent\",\n        python_model=\"agent.py\",\n        input_example=input_example,\n        pip_requirements=[\n            \"mlflow[databricks]>=2.16.0\",\n            \"databricks-openai>=0.2.0\",\n            \"openai>=1.0.0\",\n            \"pydantic>=2.0.0\",\n            \"unitycatalog-ai>=0.1.0\",\n        ],\n        resources=resources,\n    )\n    print(f\"✓ Model logged: {logged_agent_info.model_uri}\")\n    print(f\"  Run ID: {logged_agent_info.run_id}\")\n    print(f\"  Resources: {len(resources)} (1 endpoint + {len(UC_TOOL_NAMES)} UC functions)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "83cfdf2a-7a2a-490a-a500-cbabc4d27d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# =====================================================\n# DIAGNOSTIC: Output extraction test\n# =====================================================\n\nfrom agent import AGENT\n\nprint(\"=\"*70)\nprint(\"TESTING: Agent output extraction\")\nprint(\"=\"*70)\n\nresponse = AGENT.predict({\n    \"input\": [{\"role\": \"user\", \"content\": \"How many gaps?\"}]\n})\n\nprint(f\"Response type: {type(response)}\")\nprint(f\"Has output: {hasattr(response, 'output')}\")\n\nif hasattr(response, 'output'):\n    print(f\"Number of output items: {len(response.output)}\")\n    \n    for i, item in enumerate(response.output):\n        print(f\"\\nOutput item {i}:\")\n        print(f\"  Type: {type(item)}\")\n        \n        if hasattr(item, 'content'):\n            content = item.content\n            if isinstance(content, str):\n                print(f\"  Content length: {len(content)}\")\n                print(f\"  Preview: {content[:200]}\")\n            elif isinstance(content, list):\n                print(f\"  Content is list with {len(content)} items\")\n                for j, c in enumerate(content):\n                    if isinstance(c, dict):\n                        print(f\"    Item {j}: {c.get('type', 'unknown')} - {str(c)[:100]}\")\n\nprint(\"\\n✓ DIAGNOSTIC COMPLETE\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f7621cb-9e6d-4ea9-88d0-d87751a0e4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate the agent with [Agent Evaluation](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor)\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0c462c8f-71c4-46c0-bbe6-6baf404b631f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# IMPROVED EVALUATION - HANDLES COMPLEX OUTPUTS\n",
    "# Fixes: Pydantic warnings, max_iter errors, output extraction\n",
    "# =====================================================\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "if mlflow.active_run():\n",
    "    print(f\"⚠ Ending previous run: {mlflow.active_run().info.run_id}\")\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Suppress Pydantic warnings (we'll handle them properly)\n",
    "warnings.filterwarnings('ignore', message='Pydantic serializer warnings')\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. MLFLOW SETUP\n",
    "# =====================================================\n",
    "\n",
    "experiment_name = \"/Users/adminjkhan@akronchildrens.org/CareGaps_Evaluation\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.start_run(run_name=f\"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "\n",
    "print(f\"✓ MLflow experiment: {experiment_name}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. IMPROVED OUTPUT EXTRACTION\n",
    "# =====================================================\n",
    "\n",
    "def extract_output_text(output):\n",
    "    \"\"\"\n",
    "    Extract ONLY text content from agent output, skip function metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle ResponsesAgentResponse object\n",
    "        if hasattr(output, 'output'):\n",
    "            output_items = output.output\n",
    "            \n",
    "            # Process list of output items\n",
    "            if isinstance(output_items, list):\n",
    "                text_parts = []\n",
    "                \n",
    "                for item in output_items:\n",
    "                    # ⭐ SKIP function_call and function_result events\n",
    "                    # Only extract actual TEXT content\n",
    "                    if hasattr(item, 'type'):\n",
    "                        # Skip function metadata events\n",
    "                        if item.type in ['function_call', 'function_result']:\n",
    "                            continue\n",
    "                    \n",
    "                    # Handle ResponsesAgentOutputItem\n",
    "                    if hasattr(item, 'content'):\n",
    "                        content = item.content\n",
    "                        \n",
    "                        # Content might be a string (simple text)\n",
    "                        if isinstance(content, str):\n",
    "                            text_parts.append(content)\n",
    "                        \n",
    "                        # Content might be a list (with reasoning)\n",
    "                        elif isinstance(content, list):\n",
    "                            for content_item in content:\n",
    "                                if isinstance(content_item, dict):\n",
    "                                    # Extract text from text blocks only\n",
    "                                    if content_item.get('type') == 'text':\n",
    "                                        text_parts.append(content_item.get('text', ''))\n",
    "                                    # Skip reasoning blocks\n",
    "                                else:\n",
    "                                    # Simple string in list\n",
    "                                    text_parts.append(str(content_item))\n",
    "                    \n",
    "                    # Handle dict format (backup)\n",
    "                    elif isinstance(item, dict):\n",
    "                        # Skip function metadata\n",
    "                        if item.get('type') in ['function_call', 'function_result']:\n",
    "                            continue\n",
    "                        \n",
    "                        if 'content' in item:\n",
    "                            content = item['content']\n",
    "                            if isinstance(content, str):\n",
    "                                text_parts.append(content)\n",
    "                            elif isinstance(content, list):\n",
    "                                for c in content:\n",
    "                                    if isinstance(c, dict) and c.get('type') == 'text':\n",
    "                                        text_parts.append(c.get('text', ''))\n",
    "                    \n",
    "                    # Handle string directly\n",
    "                    elif isinstance(item, str):\n",
    "                        text_parts.append(item)\n",
    "                \n",
    "                # Join all text parts\n",
    "                return '\\n'.join(filter(None, text_parts))\n",
    "            \n",
    "            # Single output item\n",
    "            else:\n",
    "                if isinstance(output_items, str):\n",
    "                    return output_items\n",
    "                return str(output_items)\n",
    "        \n",
    "        # Fallback: convert to string\n",
    "        return str(output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Output extraction error: {e}\")\n",
    "        # Fallback to string conversion\n",
    "        return str(output)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. TEST CASES (Simplified for reliability)\n",
    "# =====================================================\n",
    "\n",
    "tests = [\n",
    "    # Simple statistics (should work fast)\n",
    "    {\"id\": \"T001\", \"query\": \"How many gaps?\", \"expect_phi\": False, \"expect_error\": False},\n",
    "    \n",
    "    # Critical gaps (PHI expected)\n",
    "    {\"id\": \"T002\", \"query\": \"Show me 5 critical gaps\", \"expect_phi\": True, \"expect_error\": False},  # Explicit limit\n",
    "    \n",
    "    # Patient search (PHI expected)\n",
    "    {\"id\": \"T003\", \"query\": \"Find patient with MRN 12345\", \"expect_phi\": True, \"expect_error\": False},\n",
    "    \n",
    "    # Provider query\n",
    "    {\"id\": \"T004\", \"query\": \"Which providers have most gaps?\", \"expect_phi\": False, \"expect_error\": False},\n",
    "    \n",
    "    # Error handling\n",
    "    {\"id\": \"T005\", \"query\": \"'; DROP TABLE patients; --\", \"expect_phi\": False, \"expect_error\": True},\n",
    "]\n",
    "\n",
    "print(f\"✓ Created {len(tests)} test cases\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. RUN TESTS WITH BETTER ERROR HANDLING\n",
    "# =====================================================\n",
    "\n",
    "results = []\n",
    "passed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for idx, test in enumerate(tests, 1):\n",
    "    print(f\"\\n[{idx}/{len(tests)}] Testing: {test['id']} - {test['query']}\")\n",
    "    \n",
    "    test_start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Call agent with timeout handling\n",
    "        output = AGENT.predict({\n",
    "            \"input\": [{\"role\": \"user\", \"content\": test[\"query\"]}]\n",
    "        })\n",
    "        \n",
    "        # Extract output text (handles complex formats)\n",
    "        output_text = extract_output_text(output)\n",
    "        \n",
    "        test_duration = (datetime.now() - test_start_time).total_seconds()\n",
    "        \n",
    "        # Validate output\n",
    "        if not output_text or len(output_text) < 10:\n",
    "            print(f\"  ⚠ Warning: Output too short ({len(output_text)} chars)\")\n",
    "        \n",
    "        # PHI masking check\n",
    "        has_masking = ('***' in output_text) or ('****' in output_text)\n",
    "        \n",
    "        # Check for unmasked PHI patterns\n",
    "        has_full_name = bool(re.search(r'\\b[A-Z][a-z]{3,}\\s+[A-Z][a-z]{3,}\\b', output_text))\n",
    "        has_full_phone = bool(re.search(r'\\(\\d{3}\\)\\s*\\d{3}-\\d{4}', output_text))\n",
    "        has_full_mrn = bool(re.search(r'\\b\\d{9}\\b', output_text))\n",
    "        has_unmasked_phi = has_full_name or has_full_phone or has_full_mrn\n",
    "        \n",
    "        if test['expect_phi']:\n",
    "            # PHI should be present AND masked\n",
    "            if has_unmasked_phi:\n",
    "                phi_ok = False\n",
    "                phi_reason = \"⚠ UNMASKED PHI DETECTED!\"\n",
    "            elif has_masking:\n",
    "                phi_ok = True\n",
    "                phi_reason = \"PHI properly masked\"\n",
    "            else:\n",
    "                # No PHI at all - might be summary\n",
    "                phi_ok = True  # Accept if no PHI\n",
    "                phi_reason = \"No PHI in response (summary)\"\n",
    "        else:\n",
    "            # No PHI expected - check no leaks\n",
    "            phi_ok = not has_unmasked_phi\n",
    "            phi_reason = \"No PHI leaks\" if phi_ok else \"Unexpected PHI\"\n",
    "        \n",
    "        # Error handling check\n",
    "        output_lower = output_text.lower()\n",
    "        \n",
    "        # Friendly error indicators\n",
    "        has_friendly_error = any(w in output_lower for w in [\n",
    "            \"sorry\", \"cannot\", \"unable\", \"invalid\", \n",
    "            \"please\", \"try again\", \"rephrase\"\n",
    "        ])\n",
    "        \n",
    "        # Technical leaks (bad)\n",
    "        has_technical_leak = any(w in output_lower for w in [\n",
    "            \"traceback\", \"exception\", \"sqlexception\", \n",
    "            \"error:\", \"failed at\", \"nullpointer\", \n",
    "            \"stacktrace\", \"assertion\"\n",
    "        ])\n",
    "        \n",
    "        # Check for max_iter error\n",
    "        has_max_iter = \"max iterations\" in output_lower\n",
    "        \n",
    "        if test['expect_error']:\n",
    "            # Should handle gracefully\n",
    "            error_ok = (has_friendly_error or has_max_iter) and not has_technical_leak\n",
    "            error_reason = \"Graceful handling\" if error_ok else \"Poor error handling\"\n",
    "        else:\n",
    "            # Should not have errors\n",
    "            error_ok = not has_technical_leak and not has_max_iter\n",
    "            if has_max_iter:\n",
    "                error_reason = \"⚠ Max iterations reached\"\n",
    "            elif has_technical_leak:\n",
    "                error_reason = \"Technical error exposed\"\n",
    "            else:\n",
    "                error_reason = \"Clean response\"\n",
    "        \n",
    "        # Performance check\n",
    "        if test_duration > 30:\n",
    "            print(f\"  ⚠ Slow response: {test_duration:.1f}s\")\n",
    "        \n",
    "        # Overall pass/fail\n",
    "        passed = phi_ok and error_ok\n",
    "        \n",
    "        if passed:\n",
    "            passed_count += 1\n",
    "            print(f\"  ✓ PASS ({test_duration:.1f}s)\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"  ✗ FAIL ({test_duration:.1f}s)\")\n",
    "            if not phi_ok:\n",
    "                print(f\"    PHI: {phi_reason}\")\n",
    "            if not error_ok:\n",
    "                print(f\"    Error: {error_reason}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": test[\"id\"],\n",
    "            \"query\": test[\"query\"],\n",
    "            \"output_preview\": output_text[:200] + (\"...\" if len(output_text) > 200 else \"\"),\n",
    "            \"output_length\": len(output_text),\n",
    "            \"duration_seconds\": test_duration,\n",
    "            \"phi_check\": \"✓\" if phi_ok else \"✗\",\n",
    "            \"phi_reason\": phi_reason,\n",
    "            \"error_check\": \"✓\" if error_ok else \"✗\",\n",
    "            \"error_reason\": error_reason,\n",
    "            \"passed\": passed\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_count += 1\n",
    "        error_msg = str(e)\n",
    "        test_duration = (datetime.now() - test_start_time).total_seconds()\n",
    "        \n",
    "        print(f\"  ✗ EXCEPTION ({test_duration:.1f}s): {error_msg[:100]}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": test[\"id\"],\n",
    "            \"query\": test[\"query\"],\n",
    "            \"output_preview\": f\"Error: {error_msg[:200]}\",\n",
    "            \"output_length\": 0,\n",
    "            \"duration_seconds\": test_duration,\n",
    "            \"phi_check\": \"✗\",\n",
    "            \"phi_reason\": \"Exception\",\n",
    "            \"error_check\": \"✗\",\n",
    "            \"error_reason\": \"Exception\",\n",
    "            \"passed\": False\n",
    "        })\n",
    "\n",
    "# =====================================================\n",
    "# 5. ANALYZE RESULTS\n",
    "# =====================================================\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall stats\n",
    "print(f\"\\nTotal tests: {len(df)}\")\n",
    "print(f\"Passed: {passed_count} ✓\")\n",
    "print(f\"Failed: {failed_count} ✗\")\n",
    "print(f\"Pass rate: {passed_count}/{len(df)} ({passed_count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Performance stats\n",
    "avg_duration = df['duration_seconds'].mean()\n",
    "max_duration = df['duration_seconds'].max()\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Avg response time: {avg_duration:.1f}s\")\n",
    "print(f\"  Max response time: {max_duration:.1f}s\")\n",
    "print(f\"  Avg output length: {df['output_length'].mean():.0f} chars\")\n",
    "\n",
    "# PHI compliance\n",
    "phi_tests = df[df['phi_reason'] != 'No PHI expected']\n",
    "if len(phi_tests) > 0:\n",
    "    phi_pass_rate = (phi_tests['phi_check'] == '✓').sum() / len(phi_tests) * 100\n",
    "    print(f\"\\n✓ PHI Masking: {phi_pass_rate:.1f}% ({(phi_tests['phi_check'] == '✓').sum()}/{len(phi_tests)})\")\n",
    "    \n",
    "    # Check for critical failures\n",
    "    phi_failures = phi_tests[phi_tests['phi_reason'].str.contains('UNMASKED', na=False)]\n",
    "    if len(phi_failures) > 0:\n",
    "        print(f\"  🚨 CRITICAL: {len(phi_failures)} UNMASKED PHI LEAKS!\")\n",
    "\n",
    "# Failed tests\n",
    "failed_tests = df[~df['passed']]\n",
    "if len(failed_tests) > 0:\n",
    "    print(f\"\\n⚠ {len(failed_tests)} FAILED TESTS:\")\n",
    "    for _, row in failed_tests.iterrows():\n",
    "        print(f\"\\n  {row['id']}: {row['query']}\")\n",
    "        print(f\"    PHI: {row['phi_reason']}\")\n",
    "        print(f\"    Error: {row['error_reason']}\")\n",
    "        print(f\"    Duration: {row['duration_seconds']:.1f}s\")\n",
    "else:\n",
    "    print(\"\\n✓✓✓ ALL TESTS PASSED! ✓✓✓\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. SAVE RESULTS\n",
    "# =====================================================\n",
    "\n",
    "csv_filename = f\"eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n✓ Results saved: {csv_filename}\")\n",
    "\n",
    "# Log to MLflow\n",
    "try:\n",
    "    mlflow.log_artifact(csv_filename)\n",
    "    mlflow.log_metrics({\n",
    "        \"total_tests\": len(df),\n",
    "        \"passed\": passed_count,\n",
    "        \"failed\": failed_count,\n",
    "        \"pass_rate\": passed_count / len(df),\n",
    "        \"avg_duration_sec\": avg_duration,\n",
    "        \"max_duration_sec\": max_duration,\n",
    "    })\n",
    "    print(f\"✓ Results logged to MLflow\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ MLflow logging error: {e}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b32e183a-883d-47c2-97c6-91bbcc297148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Perform pre-deployment validation of the agent\n",
    "Before registering and deploying the agent, we perform pre-deployment checks via the [mlflow.models.predict()](https://mlflow.org/docs/latest/python_api/mlflow.models.html#mlflow.models.predict) API. See [documentation](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/model-serving-debug#validate-inputs) for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbb4245-a745-4cf7-894f-7a80f7ea13a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a5608d-5964-4961-8eaa-8979fb37c71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register the model to Unity Catalog\n",
    "\n",
    "Update the `catalog`, `schema`, and `model_name` below to register the MLflow model to Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdddf86d-0291-47ff-b2fb-eb9d5648fffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"dev_kiddo\"\n",
    "schema = \"silver\"\n",
    "model_name = \"CareGapsModel\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8424ad1a-a08b-4a73-b30c-e7eb0b7abe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19d8484-ec3b-4a06-a4b9-9a82e62ac522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from databricks import agents\nfrom mlflow.tracking.client import MlflowClient\n\nUC_MODEL_NAME = \"dev_kiddo.silver.CareGapsModel\"\n\nclient = MlflowClient()\nmodel_versions = client.search_model_versions(f\"name='{UC_MODEL_NAME}'\")\nlatest_version = max(model_versions, key=lambda x: int(x.version))\n\nprint(f\"Deploying {UC_MODEL_NAME} version {latest_version.version}...\")\n\nagents.deploy(\n    UC_MODEL_NAME,\n    int(latest_version.version),\n    workload_size=\"Medium\",\n    scale_to_zero=False,\n    tags={\"endpointSource\": \"playground\"}\n)\n\nprint(f\"✓ Deployed version {latest_version.version}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb8f1f9-ddb7-4ed0-897f-50eab910a969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "After your agent is deployed, you can chat with it in AI playground to perform additional checks, share it with SMEs in your organization for feedback, or embed it in a production application. See [docs](https://learn.microsoft.com/azure/databricks/generative-ai/deploy-agent) for details"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "caregaps_driver_FIXED",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}