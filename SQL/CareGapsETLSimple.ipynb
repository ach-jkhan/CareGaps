{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d513d55-e26e-483b-8426-93b41706cf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "# Databricks notebook source\n# =====================================================\n# SIMPLE CARE GAPS ETL - BEGINNER FRIENDLY\n# Read from ADLS, Create Delta Tables\n# =====================================================\n\nprint(\"Starting Care Gaps ETL...\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Configuration (Edit these paths for your environment)\n\n# COMMAND ----------\n\n# Get parameters passed from ADF pipeline\ndbutils.widgets.text(\"run_date\", \"\", \"Run Date (yyyy-MM-dd)\")\ndbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\ndbutils.widgets.text(\"care_gaps_count\", \"0\", \"Care Gaps Row Count\")\ndbutils.widgets.text(\"appointments_count\", \"0\", \"Appointments Row Count\")\ndbutils.widgets.text(\"patient_summary_count\", \"0\", \"Patient Summary Row Count\")\ndbutils.widgets.text(\"provider_metrics_count\", \"0\", \"Provider Metrics Row Count\")\ndbutils.widgets.text(\"campaign_opportunities_count\", \"0\", \"Campaign Opportunities Row Count\")\n\nRUN_DATE = dbutils.widgets.get(\"run_date\")\nENVIRONMENT = dbutils.widgets.get(\"environment\")\n\n# If run_date not provided (manual run), use today's date\nif not RUN_DATE:\n    from datetime import datetime\n    RUN_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n\nprint(f\"Run Date: {RUN_DATE}\")\nprint(f\"Environment: {ENVIRONMENT}\")\n\n# Storage account configuration - CHANGE THESE TO YOUR VALUES\nSTORAGE_ACCOUNT = \"duse1achstdbx1\"  # Your storage account name abfss://dev@duse1achstdbx1.dfs.core.windows.net/\nCONTAINER = \"dev\"       # Your container name\nSTORAGE_KEY = \"ouqQcLrewVPACdGe5y9i6z+Qz3+Jz2TT6ivC8HCO5VNiJ/i5x3nJvE/uQplUBlUfXSsqNTg3wNZm+AStDFVQAA==\"  # Get from Azure Portal -> Storage Account -> Access Keys\n\n# Configure Spark to access ADLS\nspark.conf.set(\n    f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\",\n    STORAGE_KEY\n)\n\nprint(f\"✓ Configured access to storage account: {STORAGE_ACCOUNT}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. Define Paths (All in one place)\n\n# COMMAND ----------\n\n# Base path to your storage\nBASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n\n# Landing zone (where ADF puts Parquet files) - includes RunDate partition\nLANDING_PATH = f\"{BASE_PATH}/landing/chmca_custom/caregaps/{RUN_DATE}\"\n\nCATALOG = \"dev_kiddo\"\nBRONZE_SCHEMA = \"bronze\"\nSILVER_SCHEMA = \"silver\"\nGOLD_SCHEMA = \"gold\"\n\n# Delta Lake paths /Volumes/dev_kiddo/bronze/landing/chmca_custom/ah_eligibility_roster_mrn/raw/ah_eligibility_roster_mrn_*.parquet\nBRONZE_PATH = f\"{CATALOG}.{BRONZE_SCHEMA}\"\nSILVER_PATH = f\"{CATALOG}.{SILVER_SCHEMA}\"\nGOLD_PATH = f\"{CATALOG}.{GOLD_SCHEMA}\"\n\nprint(\"Paths configured:\")\nprint(f\"  Landing: {LANDING_PATH}\")\nprint(f\"  Bronze:  {BRONZE_PATH}\")\nprint(f\"  Silver:  {SILVER_PATH}\")\nprint(f\"  Gold:    {GOLD_PATH}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Read Parquet Files from Landing Zone\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: Reading Parquet files from landing zone...\")\nprint(\"=\"*60)\n\n# File names match ADF sink activity output names\ndf_care_gaps = spark.read.parquet(f\"{LANDING_PATH}/CareGaps_daily.parquet\")\ncare_gaps_count = df_care_gaps.count()\nprint(f\"✓ Care Gaps: {care_gaps_count:,} rows\")\n\ndf_appointments = spark.read.parquet(f\"{LANDING_PATH}/Appointments_daily.parquet\")\nappointments_count = df_appointments.count()\nprint(f\"✓ Appointments: {appointments_count:,} rows\")\n\ndf_patient_summary = spark.read.parquet(f\"{LANDING_PATH}/PatientGapsSummary_daily.parquet\")\npatient_summary_count = df_patient_summary.count()\nprint(f\"✓ Patient Summary: {patient_summary_count:,} rows\")\n\ndf_provider_metrics = spark.read.parquet(f\"{LANDING_PATH}/ProviderMetrics_daily.parquet\")\nprovider_metrics_count = df_provider_metrics.count()\nprint(f\"✓ Provider Metrics: {provider_metrics_count:,} rows\")\n\ndf_campaign_opportunities = spark.read.parquet(f\"{LANDING_PATH}/CampaignOpportunities_daily.parquet\")\ncampaign_opportunities_count = df_campaign_opportunities.count()\nprint(f\"✓ Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\n✓ All files read successfully!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 4. Create Bronze Delta Tables (Raw Data)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Creating Bronze Delta tables...\")\nprint(\"=\"*60)\n\n# Bronze: Care Gaps\n\ndf_care_gaps.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.care_gaps_daily\"\n)\nprint(\"✓ Bronze: care_gaps_daily created\")\n\ndf_appointments.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.appointments_daily\"\n)\nprint(\"✓ Bronze: appointments_daily created\")\n\ndf_patient_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.patient_summary_daily\"\n)\nprint(\"✓ Bronze: patient_summary_daily created\")\n\ndf_provider_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.provider_metrics_daily\"\n)\nprint(\"✓ Bronze: provider_metrics_daily created\")\n\ndf_campaign_opportunities.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.campaign_opportunities_daily\"\n)\nprint(\"✓ Bronze: campaign_opportunities_daily created\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5. Create Silver Delta Tables (Cleaned Data)\n\n# COMMAND ----------\n\nfrom pyspark.sql.functions import *\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Creating Silver Delta tables...\")\nprint(\"=\"*60)\n\n# Silver: Care Gaps (cleaned)\ndf_care_gaps_clean = df_care_gaps \\\n    .filter(col(\"PAT_ID\").isNotNull()) \\\n    .filter(col(\"GAP_TYPE\").isNotNull()) \\\n    .withColumn(\"PRIORITY_NAME\", \n                when(col(\"PRIORITY_LEVEL\") == 1, \"Critical\")\n                .when(col(\"PRIORITY_LEVEL\") == 2, \"Important\")\n                .otherwise(\"Routine\"))\n\ndf_care_gaps_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.care_gaps_cleaned\")\n\nsilver_care_gaps_count = df_care_gaps_clean.count()\nprint(f\"✓ Silver: care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\n\n# Silver: Patient 360 (joined data)\ndf_patient_360 = df_patient_summary.join(\n    df_appointments.groupBy(\"PAT_ID\").agg(\n        min(\"APPT_DATE\").alias(\"FIRST_APPT_DATE\"),\n        min(\"DAYS_UNTIL_APPT\").alias(\"DAYS_UNTIL_FIRST_APPT\")\n    ),\n    \"PAT_ID\",\n    \"left\"\n)\n\ndf_patient_360.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.patient_360\")\n\nsilver_patient_360_count = df_patient_360.count()\nprint(f\"✓ Silver: patient_360 ({silver_patient_360_count:,} rows)\")\n\n# Silver: Campaign Opportunities (cleaned, without LLM messages yet)\ndf_campaign_clean = df_campaign_opportunities \\\n    .filter(col(\"patient_mrn\").isNotNull()) \\\n    .filter(col(\"campaign_type\").isNotNull())\n\nsilver_campaign_count = df_campaign_clean.count()\nprint(f\"✓ Silver: campaign_opportunities cleaned ({silver_campaign_count:,} rows)\")\n\nprint(\"\\n✓ All Silver tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5b. Generate LLM Messages for Campaign Opportunities\n# MAGIC \n# MAGIC Takes each row's `suggested_prompt` and calls Llama 3.3 70B to generate\n# MAGIC a personalized 160-character SMS message. Stores the result in `llm_message`.\n\n# COMMAND ----------\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport pandas as pd\nfrom openai import OpenAI\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3b: Generating LLM messages for campaign opportunities...\")\nprint(\"=\"*60)\n\n# Check if suggested_prompt column exists\ncampaign_columns = [c.lower() for c in df_campaign_clean.columns]\n\nif \"suggested_prompt\" not in campaign_columns:\n    print(\"⚠ No 'suggested_prompt' column found — skipping LLM generation\")\n    print(\"  Writing campaign data without LLM messages...\")\n    df_campaign_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n        f\"{SILVER_PATH}.campaign_opportunities\"\n    )\nelse:\n    # Get Databricks auth token and workspace URL\n    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n    workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n\n    client = OpenAI(\n        api_key=token,\n        base_url=f\"https://{workspace_url}/serving-endpoints\"\n    )\n\n    LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n\n    def generate_message(prompt_text):\n        \"\"\"Call Llama 3.3 to generate a 160-char SMS message from the suggested prompt.\"\"\"\n        try:\n            response = client.chat.completions.create(\n                model=LLM_ENDPOINT,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": (\n                            \"You are a message writer for Akron Children's Hospital. \"\n                            \"Generate a single SMS message (160 characters max). \"\n                            \"Be cheerful and professional. Output ONLY the message text, nothing else.\"\n                        ),\n                    },\n                    {\"role\": \"user\", \"content\": prompt_text},\n                ],\n                max_tokens=80,\n                temperature=0.7,\n            )\n            msg = response.choices[0].message.content.strip()\n            # Remove quotes if the LLM wraps in quotes\n            if msg.startswith('\"') and msg.endswith('\"'):\n                msg = msg[1:-1]\n            return msg[:160]\n        except Exception as e:\n            print(f\"  LLM error: {e}\")\n            return None\n\n    # Convert to pandas for row-by-row LLM calls\n    pdf = df_campaign_clean.toPandas()\n    total_rows = len(pdf)\n    print(f\"  Processing {total_rows} opportunities...\")\n\n    # Parallel generation with ThreadPoolExecutor\n    messages = [None] * total_rows\n    prompt_indices = []\n\n    for idx, row in pdf.iterrows():\n        prompt = row.get(\"suggested_prompt\")\n        if prompt and pd.notna(prompt) and str(prompt).strip():\n            prompt_indices.append((idx, str(prompt).strip()))\n\n    print(f\"  {len(prompt_indices)} rows have suggested_prompt to process\")\n\n    completed = 0\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_idx = {\n            executor.submit(generate_message, prompt): idx\n            for idx, prompt in prompt_indices\n        }\n        for future in as_completed(future_to_idx):\n            idx = future_to_idx[future]\n            try:\n                messages[idx] = future.result()\n            except Exception as e:\n                print(f\"  Error for row {idx}: {e}\")\n            completed += 1\n            if completed % 100 == 0:\n                print(f\"  Progress: {completed}/{len(prompt_indices)} messages generated\")\n\n    pdf[\"llm_message\"] = messages\n    generated_count = sum(1 for m in messages if m is not None)\n    print(f\"✓ Generated {generated_count}/{len(prompt_indices)} LLM messages\")\n\n    # Convert back to Spark and write to Silver\n    df_campaign_with_messages = spark.createDataFrame(pdf)\n    df_campaign_with_messages.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n        f\"{SILVER_PATH}.campaign_opportunities\"\n    )\n    print(f\"✓ Silver: campaign_opportunities updated with LLM messages ({silver_campaign_count:,} rows)\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 6. Create Gold Delta Tables (Analytics)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Creating Gold Delta tables...\")\nprint(\"=\"*60)\n\n# Gold: Gap Summary by Type\ndf_gap_summary = df_care_gaps_clean.groupBy(\"GAP_TYPE\", \"PRIORITY_NAME\") \\\n    .agg(\n        count(\"*\").alias(\"TOTAL_GAPS\"),\n        countDistinct(\"PAT_ID\").alias(\"PATIENTS_AFFECTED\")\n    ) \\\n    .orderBy(\"TOTAL_GAPS\", ascending=False)\n\ndf_gap_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.gap_summary\")\n\ngold_gap_summary_count = df_gap_summary.count()\nprint(f\"✓ Gold: gap_summary ({gold_gap_summary_count:,} rows)\")\n\n# Gold: Provider Dashboard\ndf_provider_dashboard = df_provider_metrics \\\n    .withColumn(\"GAP_RATE\", col(\"TOTAL_GAPS\") / col(\"TOTAL_PATIENTS_WITH_GAPS\"))\n\ndf_provider_dashboard.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.provider_dashboard\")\n\ngold_provider_dashboard_count = df_provider_dashboard.count()\nprint(f\"✓ Gold: provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n✓ All Gold tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 7. Summary\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ETL COMPLETE - SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nRun Date: {RUN_DATE}\")\nprint(\"\\nData Loaded:\")\nprint(f\"  Care Gaps:      {care_gaps_count:,} rows\")\nprint(f\"  Appointments:   {appointments_count:,} rows\")\nprint(f\"  Patient Summary: {patient_summary_count:,} rows\")\nprint(f\"  Provider Metrics: {provider_metrics_count:,} rows\")\nprint(f\"  Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\nDelta Tables Created:\")\nprint(\"\\nBronze Layer:\")\nprint(f\"  {BRONZE_PATH}.care_gaps_daily\")\nprint(f\"  {BRONZE_PATH}.appointments_daily\")\nprint(f\"  {BRONZE_PATH}.patient_summary_daily\")\nprint(f\"  {BRONZE_PATH}.provider_metrics_daily\")\nprint(f\"  {BRONZE_PATH}.campaign_opportunities_daily\")\n\nprint(\"\\nSilver Layer:\")\nprint(f\"  {SILVER_PATH}.care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.patient_360 ({silver_patient_360_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.campaign_opportunities ({silver_campaign_count:,} rows)\")\n\nprint(\"\\nGold Layer:\")\nprint(f\"  {GOLD_PATH}.gap_summary ({gold_gap_summary_count:,} rows)\")\nprint(f\"  {GOLD_PATH}.provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ SUCCESS - All data processed!\")\nprint(\"=\"*60)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 8. Preview Data (Optional)\n\n# COMMAND ----------\n\n# Uncomment to see sample data\n\n# print(\"\\nSample Care Gaps:\")\n# display(df_care_gaps_clean.limit(10))\n\n# print(\"\\nSample Patient 360:\")\n# display(df_patient_360.limit(10))\n\n# print(\"\\nGap Summary:\")\n# display(df_gap_summary)"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CareGapsETLSimple",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}