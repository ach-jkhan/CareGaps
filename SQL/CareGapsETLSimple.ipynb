{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d513d55-e26e-483b-8426-93b41706cf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n# =====================================================\n# SIMPLE CARE GAPS ETL - BEGINNER FRIENDLY\n# Read from ADLS, Create Delta Tables\n# =====================================================\n\nprint(\"Starting Care Gaps ETL...\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Configuration (Edit these paths for your environment)\n\n# COMMAND ----------\n\n# Get parameters passed from ADF pipeline\ndbutils.widgets.text(\"run_date\", \"\", \"Run Date (yyyy-MM-dd)\")\ndbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\ndbutils.widgets.text(\"care_gaps_count\", \"0\", \"Care Gaps Row Count\")\ndbutils.widgets.text(\"appointments_count\", \"0\", \"Appointments Row Count\")\ndbutils.widgets.text(\"patient_summary_count\", \"0\", \"Patient Summary Row Count\")\ndbutils.widgets.text(\"provider_metrics_count\", \"0\", \"Provider Metrics Row Count\")\ndbutils.widgets.text(\"campaign_opportunities_count\", \"0\", \"Campaign Opportunities Row Count\")\n\nRUN_DATE = dbutils.widgets.get(\"run_date\")\nENVIRONMENT = dbutils.widgets.get(\"environment\")\n\n# If run_date not provided (manual run), use today's date\nif not RUN_DATE:\n    from datetime import datetime\n    RUN_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n\nprint(f\"Run Date: {RUN_DATE}\")\nprint(f\"Environment: {ENVIRONMENT}\")\n\n# Storage account configuration - CHANGE THESE TO YOUR VALUES\nSTORAGE_ACCOUNT = \"duse1achstdbx1\"  # Your storage account name abfss://dev@duse1achstdbx1.dfs.core.windows.net/\nCONTAINER = \"dev\"       # Your container name\nSTORAGE_KEY = \"ouqQcLrewVPACdGe5y9i6z+Qz3+Jz2TT6ivC8HCO5VNiJ/i5x3nJvE/uQplUBlUfXSsqNTg3wNZm+AStDFVQAA==\"  # Get from Azure Portal -> Storage Account -> Access Keys\n\n# Configure Spark to access ADLS\nspark.conf.set(\n    f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\",\n    STORAGE_KEY\n)\n\nprint(f\"\u2713 Configured access to storage account: {STORAGE_ACCOUNT}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. Define Paths (All in one place)\n\n# COMMAND ----------\n\n# Base path to your storage\nBASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n\n# Landing zone (where ADF puts Parquet files) - includes RunDate partition\nLANDING_PATH = f\"{BASE_PATH}/landing/chmca_custom/caregaps/{RUN_DATE}\"\n\nCATALOG = \"dev_kiddo\"\nBRONZE_SCHEMA = \"bronze\"\nSILVER_SCHEMA = \"silver\"\nGOLD_SCHEMA = \"gold\"\n\n# Delta Lake paths /Volumes/dev_kiddo/bronze/landing/chmca_custom/ah_eligibility_roster_mrn/raw/ah_eligibility_roster_mrn_*.parquet\nBRONZE_PATH = f\"{CATALOG}.{BRONZE_SCHEMA}\"\nSILVER_PATH = f\"{CATALOG}.{SILVER_SCHEMA}\"\nGOLD_PATH = f\"{CATALOG}.{GOLD_SCHEMA}\"\n\nprint(\"Paths configured:\")\nprint(f\"  Landing: {LANDING_PATH}\")\nprint(f\"  Bronze:  {BRONZE_PATH}\")\nprint(f\"  Silver:  {SILVER_PATH}\")\nprint(f\"  Gold:    {GOLD_PATH}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Read Parquet Files from Landing Zone\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: Reading Parquet files from landing zone...\")\nprint(\"=\"*60)\n\n# File names match ADF sink activity output names\ndf_care_gaps = spark.read.parquet(f\"{LANDING_PATH}/CareGaps_daily.parquet\")\ncare_gaps_count = df_care_gaps.count()\nprint(f\"\u2713 Care Gaps: {care_gaps_count:,} rows\")\n\ndf_appointments = spark.read.parquet(f\"{LANDING_PATH}/Appointments_daily.parquet\")\nappointments_count = df_appointments.count()\nprint(f\"\u2713 Appointments: {appointments_count:,} rows\")\n\ndf_patient_summary = spark.read.parquet(f\"{LANDING_PATH}/PatientGapsSummary_daily.parquet\")\npatient_summary_count = df_patient_summary.count()\nprint(f\"\u2713 Patient Summary: {patient_summary_count:,} rows\")\n\ndf_provider_metrics = spark.read.parquet(f\"{LANDING_PATH}/ProviderMetrics_daily.parquet\")\nprovider_metrics_count = df_provider_metrics.count()\nprint(f\"\u2713 Provider Metrics: {provider_metrics_count:,} rows\")\n\ndf_campaign_opportunities = spark.read.parquet(f\"{LANDING_PATH}/CampaignOpportunities_daily.parquet\")\ncampaign_opportunities_count = df_campaign_opportunities.count()\nprint(f\"\u2713 Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\n\u2713 All files read successfully!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 4. Create Bronze Delta Tables (Raw Data)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Creating Bronze Delta tables...\")\nprint(\"=\"*60)\n\n# Bronze: Care Gaps\n\ndf_care_gaps.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.care_gaps_daily\"\n)\nprint(\"\u2713 Bronze: care_gaps_daily created\")\n\ndf_appointments.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.appointments_daily\"\n)\nprint(\"\u2713 Bronze: appointments_daily created\")\n\ndf_patient_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.patient_summary_daily\"\n)\nprint(\"\u2713 Bronze: patient_summary_daily created\")\n\ndf_provider_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.provider_metrics_daily\"\n)\nprint(\"\u2713 Bronze: provider_metrics_daily created\")\n\ndf_campaign_opportunities.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.campaign_opportunities_daily\"\n)\nprint(\"\u2713 Bronze: campaign_opportunities_daily created\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5. Create Silver Delta Tables (Cleaned Data)\n\n# COMMAND ----------\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom delta.tables import DeltaTable\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Creating Silver Delta tables...\")\nprint(\"=\"*60)\n\n# Silver: Care Gaps (cleaned)\ndf_care_gaps_clean = df_care_gaps \\\n    .filter(col(\"PAT_ID\").isNotNull()) \\\n    .filter(col(\"GAP_TYPE\").isNotNull()) \\\n    .withColumn(\"PRIORITY_NAME\", \n                when(col(\"PRIORITY_LEVEL\") == 1, \"Critical\")\n                .when(col(\"PRIORITY_LEVEL\") == 2, \"Important\")\n                .otherwise(\"Routine\"))\n\ndf_care_gaps_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.care_gaps_cleaned\")\n\nsilver_care_gaps_count = df_care_gaps_clean.count()\nprint(f\"\u2713 Silver: care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\n\n# Silver: Patient 360 (joined data)\ndf_patient_360 = df_patient_summary.join(\n    df_appointments.groupBy(\"PAT_ID\").agg(\n        min(\"APPT_DATE\").alias(\"FIRST_APPT_DATE\"),\n        min(\"DAYS_UNTIL_APPT\").alias(\"DAYS_UNTIL_FIRST_APPT\")\n    ),\n    \"PAT_ID\",\n    \"left\"\n)\n\ndf_patient_360.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.patient_360\")\n\nsilver_patient_360_count = df_patient_360.count()\nprint(f\"\u2713 Silver: patient_360 ({silver_patient_360_count:,} rows)\")\n\n# ------------------------------------------------------------------\n# Silver: Campaign Opportunities \u2014 MERGE to preserve llm_message & status\n# ------------------------------------------------------------------\ncampaign_table_name = f\"{SILVER_PATH}.campaign_opportunities\"\n\ndf_campaign_clean = df_campaign_opportunities \\\n    .filter(col(\"patient_mrn\").isNotNull()) \\\n    .filter(col(\"campaign_type\").isNotNull())\n\n# Deduplicate source: keep one row per (patient_mrn, subject_mrn, campaign_type)\n# to avoid \"multiple source rows matched same target row\" MERGE error\nw = Window.partitionBy(\"patient_mrn\", \"subject_mrn\", \"campaign_type\") \\\n          .orderBy(col(\"appointment_date\").desc())\ndf_campaign_dedup = df_campaign_clean \\\n    .withColumn(\"_rn\", row_number().over(w)) \\\n    .filter(col(\"_rn\") == 1) \\\n    .drop(\"_rn\")\n\n# Check if the Silver table already exists\ntable_exists = spark.catalog.tableExists(campaign_table_name)\n\nif table_exists:\n    # MERGE: update staging columns but PRESERVE llm_message and status\n    preserve_cols = {\"llm_message\", \"status\"}\n    source_cols = [c for c in df_campaign_dedup.columns]\n    update_set = {c: f\"source.{c}\" for c in source_cols if c.lower() not in preserve_cols}\n\n    delta_table = DeltaTable.forName(spark, campaign_table_name)\n    delta_table.alias(\"target\").merge(\n        df_campaign_dedup.alias(\"source\"),\n        \"\"\"target.patient_mrn = source.patient_mrn\n           AND target.subject_mrn = source.subject_mrn\n           AND target.campaign_type = source.campaign_type\"\"\"\n    ).whenMatchedUpdate(\n        set=update_set\n    ).whenNotMatchedInsertAll(\n    ).execute()\n\n    silver_campaign_count = spark.table(campaign_table_name).count()\n    print(f\"\u2713 Silver: campaign_opportunities MERGED ({silver_campaign_count:,} rows) \u2014 llm_message & status preserved\")\nelse:\n    # First run \u2014 create the table\n    df_campaign_dedup.write.format(\"delta\").mode(\"overwrite\").saveAsTable(campaign_table_name)\n    silver_campaign_count = df_campaign_dedup.count()\n    print(f\"\u2713 Silver: campaign_opportunities CREATED ({silver_campaign_count:,} rows)\")\n\nprint(\"\\n\u2713 All Silver tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5b. Generate LLM Messages (Run Independently)\n# MAGIC \n# MAGIC **This cell is self-contained.** Run it any time to generate LLM messages\n# MAGIC for campaign opportunities that don't have one yet.\n# MAGIC \n# MAGIC - Reads directly from `dev_kiddo.silver.campaign_opportunities`\n# MAGIC - Only processes unique (patient_mrn, subject_mrn, campaign_type) combos needing messages\n# MAGIC - Uses Delta MERGE with deduplicated source to avoid conflicts\n# MAGIC - Rate-limited to 3 concurrent workers with retry for 429 errors\n# MAGIC - Safe to re-run \u2014 skips rows that already have messages\n# MAGIC\n# MAGIC **Prerequisites:** Run `%pip install openai` in a cell above if not already installed.\n\n# COMMAND ----------\n\nimport os\nimport time\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport pandas as pd\nfrom openai import OpenAI\nfrom delta.tables import DeltaTable\n\n# Suppress noisy MLflow tracing warnings\nlogging.getLogger(\"mlflow.tracing\").setLevel(logging.ERROR)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING LLM MESSAGES FOR CAMPAIGN OPPORTUNITIES\")\nprint(\"=\"*60)\n\n# ---- Configuration ----\nCAMPAIGN_TABLE = \"dev_kiddo.silver.campaign_opportunities\"\nLLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\nMAX_WORKERS = 3          # Stay under workspace QPS limit\nMAX_RETRIES = 5          # Retry on 429 rate-limit errors\nBASE_DELAY = 2.0         # Initial backoff delay in seconds\n\n# ---- Read UNIQUE rows that need messages (with full context) ----\ndf_needs_messages = spark.sql(f\"\"\"\n    SELECT patient_mrn, subject_mrn, campaign_type,\n           FIRST(patient_name) AS patient_name,\n           FIRST(subject_name) AS subject_name,\n           FIRST(appointment_date) AS appointment_date,\n           FIRST(appointment_location) AS appointment_location,\n           FIRST(has_asthma) AS has_asthma,\n           FIRST(last_flu_vaccine_date) AS last_flu_vaccine_date,\n           FIRST(llm_prompt_context) AS llm_prompt_context\n    FROM {CAMPAIGN_TABLE}\n    WHERE llm_prompt_context IS NOT NULL\n      AND TRIM(llm_prompt_context) != ''\n      AND (llm_message IS NULL OR TRIM(llm_message) = '')\n    GROUP BY patient_mrn, subject_mrn, campaign_type\n\"\"\")\n\nrows_to_process = df_needs_messages.count()\nprint(f\"  Unique rows needing LLM messages: {rows_to_process}\")\n\nif rows_to_process == 0:\n    print(\"\u2713 All rows already have LLM messages \u2014 nothing to do\")\nelse:\n    # ---- Set up LLM client ----\n    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n    workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n\n    client = OpenAI(\n        api_key=token,\n        base_url=f\"https://{workspace_url}/serving-endpoints\"\n    )\n\n    # ---- Flu Vaccine Piggybacking System Prompt ----\n    SYSTEM_PROMPT = (\n        \"You are a message writer for Akron Children's Hospital's \"\n        \"Flu Vaccine Piggybacking campaign. The goal: when one child in a \"\n        \"household has an upcoming appointment, we message the parent to \"\n        \"bring a SIBLING who is overdue for their flu vaccine to that same visit. \"\n        \"Generate a single SMS message (160 characters max). \"\n        \"Be cheerful, positive, and professional. \"\n        \"The message should focus on the FLU VACCINE opportunity for the sibling, \"\n        \"NOT on the existing appointment itself. \"\n        \"Output ONLY the message text, nothing else.\"\n    )\n\n    def build_user_prompt(row):\n        \"\"\"Build a structured prompt from opportunity row data.\"\"\"\n        patient = row.get(\"patient_name\") or \"your child\"\n        subject = row.get(\"subject_name\") or \"\"\n        appt_date = str(row.get(\"appointment_date\") or \"\")\n        appt_loc = str(row.get(\"appointment_location\") or \"\")\n        has_asthma = str(row.get(\"has_asthma\") or \"N\").upper() == \"Y\"\n        last_vax = str(row.get(\"last_flu_vaccine_date\") or \"\")\n        # Sibling piggybacking = patient is different from subject\n        is_sibling = (patient.strip().lower() != subject.strip().lower()) if subject else False\n\n        if is_sibling:\n            prompt = (\n                f\"{patient} is overdue for a flu vaccine. \"\n                f\"Their household member {subject} has an upcoming appointment \"\n                f\"at {appt_loc} on {appt_date}. \"\n                f\"Write an SMS to the parent suggesting they bring {patient} \"\n                f\"for a flu shot during {subject}'s visit.\"\n            )\n        else:\n            prompt = (\n                f\"{patient} is overdue for a flu vaccine and has an upcoming \"\n                f\"appointment at {appt_loc} on {appt_date}. \"\n                f\"Write an SMS suggesting they get their flu shot during that visit.\"\n            )\n\n        if last_vax and last_vax.lower() not in (\"\", \"none\", \"never\", \"nan\", \"nat\"):\n            prompt += f\" Their last flu vaccine was {last_vax} \u2014 remind them each vaccine only protects for one season.\"\n\n        if has_asthma:\n            prompt += f\" IMPORTANT: {patient} has asthma, which puts them at higher risk for severe flu complications. Mention this.\"\n\n        return prompt\n\n    def generate_message(row_dict):\n        \"\"\"Call Llama 3.3 with retry on rate-limit errors.\"\"\"\n        user_prompt = build_user_prompt(row_dict)\n        for attempt in range(MAX_RETRIES):\n            try:\n                response = client.chat.completions.create(\n                    model=LLM_ENDPOINT,\n                    messages=[\n                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                        {\"role\": \"user\", \"content\": user_prompt},\n                    ],\n                    max_tokens=80,\n                    temperature=0.7,\n                )\n                msg = response.choices[0].message.content.strip()\n                if msg.startswith('\"') and msg.endswith('\"'):\n                    msg = msg[1:-1]\n                return msg[:160]\n            except Exception as e:\n                if \"429\" in str(e) or \"RATE_LIMIT\" in str(e).upper():\n                    delay = BASE_DELAY * (2 ** attempt)\n                    time.sleep(delay)\n                    continue\n                print(f\"  LLM error: {e}\")\n                return None\n        print(f\"  Gave up after {MAX_RETRIES} retries (rate limited)\")\n        return None\n\n    # ---- Generate messages with limited concurrency ----\n    pdf = df_needs_messages.toPandas()\n    messages = [None] * len(pdf)\n    row_items = []\n\n    for idx, row in pdf.iterrows():\n        prompt_ctx = row.get(\"llm_prompt_context\")\n        if prompt_ctx and pd.notna(prompt_ctx) and str(prompt_ctx).strip():\n            row_items.append((idx, row.to_dict()))\n\n    print(f\"  Processing {len(row_items)} prompts with {MAX_WORKERS} workers...\")\n\n    completed = 0\n    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        future_to_idx = {\n            executor.submit(generate_message, row_dict): idx\n            for idx, row_dict in row_items\n        }\n        for future in as_completed(future_to_idx):\n            idx = future_to_idx[future]\n            try:\n                messages[idx] = future.result()\n            except Exception as e:\n                print(f\"  Error for row {idx}: {e}\")\n            completed += 1\n            if completed % 50 == 0:\n                print(f\"  Progress: {completed}/{len(row_items)}\")\n\n    pdf[\"llm_message\"] = messages\n    generated_count = sum(1 for m in messages if m is not None)\n    print(f\"  Generated {generated_count}/{len(row_items)} messages\")\n\n    # ---- MERGE deduplicated source into Delta table ----\n    df_updates = spark.createDataFrame(\n        pdf[pdf[\"llm_message\"].notna()][[\"patient_mrn\", \"subject_mrn\", \"campaign_type\", \"llm_message\"]]\n    )\n\n    delta_table = DeltaTable.forName(spark, CAMPAIGN_TABLE)\n    delta_table.alias(\"target\").merge(\n        df_updates.alias(\"source\"),\n        \"\"\"target.patient_mrn = source.patient_mrn\n           AND target.subject_mrn = source.subject_mrn\n           AND target.campaign_type = source.campaign_type\"\"\"\n    ).whenMatchedUpdate(\n        set={\"llm_message\": \"source.llm_message\"}\n    ).execute()\n\n    print(f\"\u2713 Updated rows with LLM messages in {CAMPAIGN_TABLE}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 6. Create Gold Delta Tables (Analytics)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Creating Gold Delta tables...\")\nprint(\"=\"*60)\n\n# Gold: Gap Summary by Type\ndf_gap_summary = df_care_gaps_clean.groupBy(\"GAP_TYPE\", \"PRIORITY_NAME\") \\\n    .agg(\n        count(\"*\").alias(\"TOTAL_GAPS\"),\n        countDistinct(\"PAT_ID\").alias(\"PATIENTS_AFFECTED\")\n    ) \\\n    .orderBy(\"TOTAL_GAPS\", ascending=False)\n\ndf_gap_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.gap_summary\")\n\ngold_gap_summary_count = df_gap_summary.count()\nprint(f\"\u2713 Gold: gap_summary ({gold_gap_summary_count:,} rows)\")\n\n# Gold: Provider Dashboard\ndf_provider_dashboard = df_provider_metrics \\\n    .withColumn(\"GAP_RATE\", col(\"TOTAL_GAPS\") / col(\"TOTAL_PATIENTS_WITH_GAPS\"))\n\ndf_provider_dashboard.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.provider_dashboard\")\n\ngold_provider_dashboard_count = df_provider_dashboard.count()\nprint(f\"\u2713 Gold: provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n\u2713 All Gold tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 7. Summary\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ETL COMPLETE - SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nRun Date: {RUN_DATE}\")\nprint(\"\\nData Loaded:\")\nprint(f\"  Care Gaps:      {care_gaps_count:,} rows\")\nprint(f\"  Appointments:   {appointments_count:,} rows\")\nprint(f\"  Patient Summary: {patient_summary_count:,} rows\")\nprint(f\"  Provider Metrics: {provider_metrics_count:,} rows\")\nprint(f\"  Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\nDelta Tables Created:\")\nprint(\"\\nBronze Layer:\")\nprint(f\"  {BRONZE_PATH}.care_gaps_daily\")\nprint(f\"  {BRONZE_PATH}.appointments_daily\")\nprint(f\"  {BRONZE_PATH}.patient_summary_daily\")\nprint(f\"  {BRONZE_PATH}.provider_metrics_daily\")\nprint(f\"  {BRONZE_PATH}.campaign_opportunities_daily\")\n\nprint(\"\\nSilver Layer:\")\nprint(f\"  {SILVER_PATH}.care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.patient_360 ({silver_patient_360_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.campaign_opportunities ({silver_campaign_count:,} rows)\")\n\nprint(\"\\nGold Layer:\")\nprint(f\"  {GOLD_PATH}.gap_summary ({gold_gap_summary_count:,} rows)\")\nprint(f\"  {GOLD_PATH}.provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2713 SUCCESS - All data processed!\")\nprint(\"=\"*60)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 8. Preview Data (Optional)\n\n# COMMAND ----------\n\n# Uncomment to see sample data\n\n# print(\"\\nSample Care Gaps:\")\n# display(df_care_gaps_clean.limit(10))\n\n# print(\"\\nSample Patient 360:\")\n# display(df_patient_360.limit(10))\n\n# print(\"\\nGap Summary:\")\n# display(df_gap_summary)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CareGapsETLSimple",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}