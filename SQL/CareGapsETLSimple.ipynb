{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d513d55-e26e-483b-8426-93b41706cf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n# =====================================================\n# SIMPLE CARE GAPS ETL - BEGINNER FRIENDLY\n# Read from ADLS, Create Delta Tables\n# =====================================================\n\nprint(\"Starting Care Gaps ETL...\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 1. Configuration (Edit these paths for your environment)\n\n# COMMAND ----------\n\n# Get parameters passed from ADF pipeline\ndbutils.widgets.text(\"run_date\", \"\", \"Run Date (yyyy-MM-dd)\")\ndbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\ndbutils.widgets.text(\"care_gaps_count\", \"0\", \"Care Gaps Row Count\")\ndbutils.widgets.text(\"appointments_count\", \"0\", \"Appointments Row Count\")\ndbutils.widgets.text(\"patient_summary_count\", \"0\", \"Patient Summary Row Count\")\ndbutils.widgets.text(\"provider_metrics_count\", \"0\", \"Provider Metrics Row Count\")\ndbutils.widgets.text(\"campaign_opportunities_count\", \"0\", \"Campaign Opportunities Row Count\")\n\nRUN_DATE = dbutils.widgets.get(\"run_date\")\nENVIRONMENT = dbutils.widgets.get(\"environment\")\n\n# If run_date not provided (manual run), use today's date\nif not RUN_DATE:\n    from datetime import datetime\n    RUN_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n\nprint(f\"Run Date: {RUN_DATE}\")\nprint(f\"Environment: {ENVIRONMENT}\")\n\n# Storage account configuration - CHANGE THESE TO YOUR VALUES\nSTORAGE_ACCOUNT = \"duse1achstdbx1\"  # Your storage account name abfss://dev@duse1achstdbx1.dfs.core.windows.net/\nCONTAINER = \"dev\"       # Your container name\nSTORAGE_KEY = \"ouqQcLrewVPACdGe5y9i6z+Qz3+Jz2TT6ivC8HCO5VNiJ/i5x3nJvE/uQplUBlUfXSsqNTg3wNZm+AStDFVQAA==\"  # Get from Azure Portal -> Storage Account -> Access Keys\n\n# Configure Spark to access ADLS\nspark.conf.set(\n    f\"fs.azure.account.key.{STORAGE_ACCOUNT}.dfs.core.windows.net\",\n    STORAGE_KEY\n)\n\nprint(f\"\u2713 Configured access to storage account: {STORAGE_ACCOUNT}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 2. Define Paths (All in one place)\n\n# COMMAND ----------\n\n# Base path to your storage\nBASE_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net\"\n\n# Landing zone (where ADF puts Parquet files) - includes RunDate partition\nLANDING_PATH = f\"{BASE_PATH}/landing/chmca_custom/caregaps/{RUN_DATE}\"\n\nCATALOG = \"dev_kiddo\"\nBRONZE_SCHEMA = \"bronze\"\nSILVER_SCHEMA = \"silver\"\nGOLD_SCHEMA = \"gold\"\n\n# Delta Lake paths /Volumes/dev_kiddo/bronze/landing/chmca_custom/ah_eligibility_roster_mrn/raw/ah_eligibility_roster_mrn_*.parquet\nBRONZE_PATH = f\"{CATALOG}.{BRONZE_SCHEMA}\"\nSILVER_PATH = f\"{CATALOG}.{SILVER_SCHEMA}\"\nGOLD_PATH = f\"{CATALOG}.{GOLD_SCHEMA}\"\n\nprint(\"Paths configured:\")\nprint(f\"  Landing: {LANDING_PATH}\")\nprint(f\"  Bronze:  {BRONZE_PATH}\")\nprint(f\"  Silver:  {SILVER_PATH}\")\nprint(f\"  Gold:    {GOLD_PATH}\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 3. Read Parquet Files from Landing Zone\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: Reading Parquet files from landing zone...\")\nprint(\"=\"*60)\n\n# File names match ADF sink activity output names\ndf_care_gaps = spark.read.parquet(f\"{LANDING_PATH}/CareGaps_daily.parquet\")\ncare_gaps_count = df_care_gaps.count()\nprint(f\"\u2713 Care Gaps: {care_gaps_count:,} rows\")\n\ndf_appointments = spark.read.parquet(f\"{LANDING_PATH}/Appointments_daily.parquet\")\nappointments_count = df_appointments.count()\nprint(f\"\u2713 Appointments: {appointments_count:,} rows\")\n\ndf_patient_summary = spark.read.parquet(f\"{LANDING_PATH}/PatientGapsSummary_daily.parquet\")\npatient_summary_count = df_patient_summary.count()\nprint(f\"\u2713 Patient Summary: {patient_summary_count:,} rows\")\n\ndf_provider_metrics = spark.read.parquet(f\"{LANDING_PATH}/ProviderMetrics_daily.parquet\")\nprovider_metrics_count = df_provider_metrics.count()\nprint(f\"\u2713 Provider Metrics: {provider_metrics_count:,} rows\")\n\ndf_campaign_opportunities = spark.read.parquet(f\"{LANDING_PATH}/CampaignOpportunities_daily.parquet\")\ncampaign_opportunities_count = df_campaign_opportunities.count()\nprint(f\"\u2713 Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\n\u2713 All files read successfully!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 4. Create Bronze Delta Tables (Raw Data)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: Creating Bronze Delta tables...\")\nprint(\"=\"*60)\n\n# Bronze: Care Gaps\n\ndf_care_gaps.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.care_gaps_daily\"\n)\nprint(\"\u2713 Bronze: care_gaps_daily created\")\n\ndf_appointments.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.appointments_daily\"\n)\nprint(\"\u2713 Bronze: appointments_daily created\")\n\ndf_patient_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.patient_summary_daily\"\n)\nprint(\"\u2713 Bronze: patient_summary_daily created\")\n\ndf_provider_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.provider_metrics_daily\"\n)\nprint(\"\u2713 Bronze: provider_metrics_daily created\")\n\ndf_campaign_opportunities.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n    f\"{BRONZE_PATH}.campaign_opportunities_daily\"\n)\nprint(\"\u2713 Bronze: campaign_opportunities_daily created\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 5. Create Silver Delta Tables (Cleaned Data)\n\n# COMMAND ----------\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom delta.tables import DeltaTable\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 3: Creating Silver Delta tables...\")\nprint(\"=\"*60)\n\n# Silver: Care Gaps (cleaned)\ndf_care_gaps_clean = df_care_gaps \\\n    .filter(col(\"PAT_ID\").isNotNull()) \\\n    .filter(col(\"GAP_TYPE\").isNotNull()) \\\n    .withColumn(\"PRIORITY_NAME\", \n                when(col(\"PRIORITY_LEVEL\") == 1, \"Critical\")\n                .when(col(\"PRIORITY_LEVEL\") == 2, \"Important\")\n                .otherwise(\"Routine\"))\n\ndf_care_gaps_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.care_gaps_cleaned\")\n\nsilver_care_gaps_count = df_care_gaps_clean.count()\nprint(f\"\u2713 Silver: care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\n\n# Silver: Patient 360 (joined data)\ndf_patient_360 = df_patient_summary.join(\n    df_appointments.groupBy(\"PAT_ID\").agg(\n        min(\"APPT_DATE\").alias(\"FIRST_APPT_DATE\"),\n        min(\"DAYS_UNTIL_APPT\").alias(\"DAYS_UNTIL_FIRST_APPT\")\n    ),\n    \"PAT_ID\",\n    \"left\"\n)\n\ndf_patient_360.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_PATH}.patient_360\")\n\nsilver_patient_360_count = df_patient_360.count()\nprint(f\"\u2713 Silver: patient_360 ({silver_patient_360_count:,} rows)\")\n\n# ------------------------------------------------------------------\n# Silver: Campaign Opportunities \u2014 MERGE to preserve llm_message & status\n# ------------------------------------------------------------------\ncampaign_table_name = f\"{SILVER_PATH}.campaign_opportunities\"\n\ndf_campaign_clean = df_campaign_opportunities \\\n    .filter(col(\"patient_mrn\").isNotNull()) \\\n    .filter(col(\"campaign_type\").isNotNull())\n\n# Deduplicate source: keep one row per (patient_mrn, subject_mrn, campaign_type)\n# to avoid \"multiple source rows matched same target row\" MERGE error\nw = Window.partitionBy(\"patient_mrn\", \"subject_mrn\", \"campaign_type\") \\\n          .orderBy(col(\"appointment_date\").desc())\ndf_campaign_dedup = df_campaign_clean \\\n    .withColumn(\"_rn\", row_number().over(w)) \\\n    .filter(col(\"_rn\") == 1) \\\n    .drop(\"_rn\")\n\n# Check if the Silver table already exists\ntable_exists = spark.catalog.tableExists(campaign_table_name)\n\nif table_exists:\n    # MERGE: update staging columns but PRESERVE llm_message and status\n    preserve_cols = {\"llm_message\", \"status\"}\n    source_cols = [c for c in df_campaign_dedup.columns]\n    update_set = {c: f\"source.{c}\" for c in source_cols if c.lower() not in preserve_cols}\n\n    delta_table = DeltaTable.forName(spark, campaign_table_name)\n    delta_table.alias(\"target\").merge(\n        df_campaign_dedup.alias(\"source\"),\n        \"\"\"target.patient_mrn = source.patient_mrn\n           AND target.subject_mrn = source.subject_mrn\n           AND target.campaign_type = source.campaign_type\"\"\"\n    ).whenMatchedUpdate(\n        set=update_set\n    ).whenNotMatchedInsertAll(\n    ).execute()\n\n    silver_campaign_count = spark.table(campaign_table_name).count()\n    print(f\"\u2713 Silver: campaign_opportunities MERGED ({silver_campaign_count:,} rows) \u2014 llm_message & status preserved\")\nelse:\n    # First run \u2014 create the table\n    df_campaign_dedup.write.format(\"delta\").mode(\"overwrite\").saveAsTable(campaign_table_name)\n    silver_campaign_count = df_campaign_dedup.count()\n    print(f\"\u2713 Silver: campaign_opportunities CREATED ({silver_campaign_count:,} rows)\")\n\nprint(\"\\n\u2713 All Silver tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 6. Create Gold Delta Tables (Analytics)\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 4: Creating Gold Delta tables...\")\nprint(\"=\"*60)\n\n# Gold: Gap Summary by Type\ndf_gap_summary = df_care_gaps_clean.groupBy(\"GAP_TYPE\", \"PRIORITY_NAME\") \\\n    .agg(\n        count(\"*\").alias(\"TOTAL_GAPS\"),\n        countDistinct(\"PAT_ID\").alias(\"PATIENTS_AFFECTED\")\n    ) \\\n    .orderBy(\"TOTAL_GAPS\", ascending=False)\n\ndf_gap_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.gap_summary\")\n\ngold_gap_summary_count = df_gap_summary.count()\nprint(f\"\u2713 Gold: gap_summary ({gold_gap_summary_count:,} rows)\")\n\n# Gold: Provider Dashboard\ndf_provider_dashboard = df_provider_metrics \\\n    .withColumn(\"GAP_RATE\", col(\"TOTAL_GAPS\") / col(\"TOTAL_PATIENTS_WITH_GAPS\"))\n\ndf_provider_dashboard.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{GOLD_PATH}.provider_dashboard\")\n\ngold_provider_dashboard_count = df_provider_dashboard.count()\nprint(f\"\u2713 Gold: provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n\u2713 All Gold tables created!\")\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 7. Summary\n\n# COMMAND ----------\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ETL COMPLETE - SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nRun Date: {RUN_DATE}\")\nprint(\"\\nData Loaded:\")\nprint(f\"  Care Gaps:      {care_gaps_count:,} rows\")\nprint(f\"  Appointments:   {appointments_count:,} rows\")\nprint(f\"  Patient Summary: {patient_summary_count:,} rows\")\nprint(f\"  Provider Metrics: {provider_metrics_count:,} rows\")\nprint(f\"  Campaign Opportunities: {campaign_opportunities_count:,} rows\")\n\nprint(\"\\nDelta Tables Created:\")\nprint(\"\\nBronze Layer:\")\nprint(f\"  {BRONZE_PATH}.care_gaps_daily\")\nprint(f\"  {BRONZE_PATH}.appointments_daily\")\nprint(f\"  {BRONZE_PATH}.patient_summary_daily\")\nprint(f\"  {BRONZE_PATH}.provider_metrics_daily\")\nprint(f\"  {BRONZE_PATH}.campaign_opportunities_daily\")\n\nprint(\"\\nSilver Layer:\")\nprint(f\"  {SILVER_PATH}.care_gaps_cleaned ({silver_care_gaps_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.patient_360 ({silver_patient_360_count:,} rows)\")\nprint(f\"  {SILVER_PATH}.campaign_opportunities ({silver_campaign_count:,} rows)\")\n\nprint(\"\\nGold Layer:\")\nprint(f\"  {GOLD_PATH}.gap_summary ({gold_gap_summary_count:,} rows)\")\nprint(f\"  {GOLD_PATH}.provider_dashboard ({gold_provider_dashboard_count:,} rows)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2713 SUCCESS - All data processed!\")\nprint(\"=\"*60)\n\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC ## 8. Preview Data (Optional)\n\n# COMMAND ----------\n\n# Uncomment to see sample data\n\n# print(\"\\nSample Care Gaps:\")\n# display(df_care_gaps_clean.limit(10))\n\n# print(\"\\nSample Patient 360:\")\n# display(df_patient_360.limit(10))\n\n# print(\"\\nGap Summary:\")\n# display(df_gap_summary)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CareGapsETLSimple",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}